[
  {
    "objectID": "introduccion.html",
    "href": "introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "Carga de paquetes:\n\nlibrary(tidyverse)\nlibrary(arm)\nlibrary(readr)\n\nEjemplo de datos con medidas repetidas:\nConjunto de datos longitudinal de unos 2000 adolescentes australianos cuyos patrones de consumo de tabaco se registraron cada seis meses (a través de un cuestionario) durante un período de tres años.\nCarga de datos:\n\nsmoking &lt;- read_delim('./data/ARM_Data/smoking/smoke_pub.dat') \n\nRows: 8730 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\ndbl (5): newid, sex(1=F), parsmk, wave, smkreg\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsmoking &lt;- smoking %&gt;% rename(sex = `sex(1=F)`) %&gt;%\n  mutate(sex = factor(sex))\n\nY graficamos la prevalencia en del fumado en cada una de las aplicaciones del cuestionario (wave) para cada uno de los sexos:\n\nprevalencia &lt;- smoking %&gt;%\n  group_by(sex,wave) %&gt;%\n  summarise(prevalencia = mean(smkreg)) \n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\nggplot()+geom_line(data=prevalencia,aes(x=wave,y=prevalencia,color = sex)\n                  )+theme_minimal()+labs(title = 'Prevalencia de fumado por sexo',\n                                          x = 'Aplicación del cuestionario',\n                                          y = 'Prevalencia de fumado')\n\n\n\n\n\n\n\n\nTambién podemos separar la información propia del nivel 1 (waves) y los de nivel más agrupado o nivel 2 (individuos):\n\nsmoking_ind &lt;- smoking %&gt;% \n  dplyr::select(wave,newid,smkreg) %&gt;%\n  arrange(wave,newid)\n\nhead(smoking_ind)\n\n# A tibble: 6 × 3\n   wave newid smkreg\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1     1      0\n2     1     2      0\n3     1     3      0\n4     1     4      0\n5     1     5      0\n6     1     6      0\n\n\n\nsmoking_group &lt;- smoking %&gt;% \n  dplyr::select(newid,sex,parsmk) %&gt;%\n  distinct()\n\nhead(smoking_group)\n\n# A tibble: 6 × 3\n  newid sex   parsmk\n  &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;\n1     1 1          0\n2     2 0          0\n3     3 1          0\n4     4 1          0\n5     5 0          0\n6     6 0          0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelos Mixtos",
    "section": "",
    "text": "Prefacio\nNotas computacionales de clase para el curso SP1653, Modelos Mixtos.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "MLMbasicos.html",
    "href": "MLMbasicos.html",
    "title": "2  Modelos Lineales Multinivel básicos",
    "section": "",
    "text": "2.1 Pooling completo vs no-pooling\nCarga de datos de radon y depuración:\nradon &lt;- read_delim('./data/ARM_Data/radon/srrs2.dat',trim_ws = TRUE)\n\nRows: 12777 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): state, state2, zip, basement, rep, wave, starttm, stoptm, startdt,...\ndbl (13): idnum, stfips, region, typebldg, floor, room, stratum, activity, p...\nlgl  (1): windoor\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nradon_data &lt;- radon %&gt;%\n  filter(state == \"MN\") %&gt;%\n  mutate(\n    radon = activity,\n    log_radon = log(if_else(activity == 0, 0.1, activity)),\n    floor = floor,  # 0 for basement, 1 for first floor\n    county_name = as.vector(county)\n  ) %&gt;%\n  group_by(county_name) %&gt;%\n  mutate(\n    county = cur_group_id()\n  ) %&gt;%\n  ungroup()\nBoxplot de los niveles de radon por ciudad:\nmean_radon &lt;- mean(radon_data$log_radon, na.rm = TRUE)\n\nggplot(radon_data, aes(x = county_name, y = log_radon)) +\n  geom_boxplot() +\n  geom_abline(intercept = mean_radon, slope = 0, color = \"red\") +\n  labs(title = \"Boxplot of Log Radon Levels by City\",\n       x = \"City\",\n       y = \"Log Radon Level\") +\n  theme_minimal()\nBoxplot de los niveles de radon por ciudad ordenados por tamaño de muestra:\ntabla_ns &lt;- radon_data %&gt;% group_by(county_name) %&gt;% summarise(n = n())  \n\nradon_data &lt;- radon_data %&gt;% \n  left_join(tabla_ns, by = \"county_name\")\n\n\nradon_data &lt;- radon_data %&gt;%\n  mutate(county_name = factor(county_name, levels = tabla_ns$county_name[order(tabla_ns$n)]))\n\nggplot(radon_data, aes(x = county_name, y = log_radon)) +\n  geom_boxplot() +\n  geom_abline(intercept = mean_radon, slope = 0, color = \"red\") +\n  labs(title = \"Boxplot of Log Radon Levels by County (Sorted by Sample Size)\",\n       x = \"City\",\n       y = \"Log Radon Level\") +\n  theme_minimal()\nBoxplot de los niveles de radon por ciudad usando partial-pooling (Multinivel):\nlmer_radon &lt;- lmer(log_radon ~ 1 + (1|county_name), data = radon_data)\n\nradon_data &lt;- radon_data %&gt;% \n  mutate(log_radon_pred = predict(lmer_radon))\n\nggplot(radon_data, aes(x = county_name, y = log_radon_pred)) +\n  geom_boxplot() +\n  geom_abline(intercept = mean_radon, slope = 0, color = \"red\") +\n  labs(title = \"Boxplot of Log Radon Levels by City (Predicted)\",\n       x = \"City\",\n       y = \"Log Radon Level (Predicted)\") +\n  theme_minimal()\nPooling completo vs no-pooling con una covariable:\nmuestra_counties &lt;- c('LAC QUI PARLE', 'AITKIN','KOOCHICHING',\n                      'DOUGLAS','CLAY','STEARNS','RAMSEY','ST LOUIS')\n\nradon_data_r &lt;- radon_data %&gt;% mutate(county_s = as.character(county_name)) %&gt;%\n  filter(county_s %in% muestra_counties)\n\n\nregresion_pooling &lt;- lm(log_radon ~ floor, data = radon_data_r)\nregresion_no_pooling &lt;- lm(log_radon ~ -1+floor+county_name, data = radon_data_r)\n\n\ncoef_df &lt;- coef(regresion_no_pooling)\n\ncoef_data &lt;- data.frame(\n  county_s = sub(\"county\", \"\", names(coef_df)[grep(\"county\", names(coef_df))]),\n  slope = coef_df[grep(\"floor\", names(coef_df))],\n  intercept = coef_df[grep(\"county\", names(coef_df))]\n)\n\nWarning in data.frame(county_s = sub(\"county\", \"\",\nnames(coef_df)[grep(\"county\", : row names were found from a short variable and\nhave been discarded\n\nradon_data_r &lt;- radon_data_r %&gt;%\n  left_join(coef_data, by = \"county_s\")\n\nggplot(data = radon_data_r) +\n  geom_point(aes(x = floor, y = log_radon)) +\n  geom_abline(aes(intercept = intercept, slope = slope), color = \"blue\") +  # Specific regression lines for each county\n  geom_abline(intercept = coef(regresion_pooling)[1], slope = coef(regresion_pooling)[2], color = \"red\") +  # General regression line\n  facet_wrap(~county_s) +\n  labs(title = \"Radon Levels by Floor with Specific and General Regression Lines\",\n       x = \"Floor\",\n       y = \"Log Radon Level\") +\n  theme_minimal()\n\nWarning: Removed 209 rows containing missing values or values outside the scale range\n(`geom_abline()`).\nPooling parcial con una covariable (Multinivel):\nlmer_radon_floor &lt;- lmer(log_radon ~ floor + (1|county_name), \n                         data = radon_data_r)\n\n\nfixed_effect &lt;- fixef(lmer_radon_floor)\nrandom_effects &lt;- ranef(lmer_radon_floor)$county\n\n\ncoef_data_lmer &lt;- data.frame(\n  county_s = rownames(random_effects),\n  intercept_lmer = fixed_effect[1] + random_effects[, 1], \n  slope_lmer = fixed_effect[2]  \n)\n\nWarning in data.frame(county_s = rownames(random_effects), intercept_lmer =\nfixed_effect[1] + : row names were found from a short variable and have been\ndiscarded\n\nradon_data_r &lt;- radon_data_r %&gt;%\n  left_join(coef_data_lmer, by = \"county_s\")\n\nggplot(data = radon_data_r) +\n  geom_point(aes(x = floor, y = log_radon)) +\n  geom_abline(aes(intercept = intercept_lmer, slope = slope_lmer), color = \"blue\") +  # Specific regression lines for each county\n  geom_abline(intercept = fixed_effect[1], slope = fixed_effect[2], color = \"red\") +  # General regression line (fixed effects only)\n  facet_wrap(~county_s) +\n  labs(title = \"Radon Levels by Floor with Specific and General Regression Lines\",\n       x = \"Floor\",\n       y = \"Log Radon Level\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelos Lineales Multinivel básicos</span>"
    ]
  },
  {
    "objectID": "MLMbasicos.html#modelo-multinivel-pooling-parcial",
    "href": "MLMbasicos.html#modelo-multinivel-pooling-parcial",
    "title": "2  Modelos Lineales Multinivel básicos",
    "section": "2.2 Modelo multinivel (pooling parcial)",
    "text": "2.2 Modelo multinivel (pooling parcial)\nModelo con intercepto variable por condado y piso\n\nM1 &lt;- lmer (log_radon ~ floor + (1 | county_name),data = radon_data)\ndisplay (M1)\n\nlmer(formula = log_radon ~ floor + (1 | county_name), data = radon_data)\n            coef.est coef.se\n(Intercept)  1.46     0.05  \nfloor       -0.69     0.07  \n\nError terms:\n Groups      Name        Std.Dev.\n county_name (Intercept) 0.33    \n Residual                0.76    \n---\nnumber of obs: 919, groups: county_name, 85\nAIC = 2179.3, DIC = 2156\ndeviance = 2163.7 \n\n\nCoeficientes estimados de regresion\n\ncoef (M1)\n\n$county_name\n                  (Intercept)      floor\nMAHNOMEN            1.4456250 -0.6929937\nMURRAY              1.6253581 -0.6929937\nWILKIN              1.5835784 -0.6929937\nCOOK                1.2432992 -0.6929937\nFILLMORE            1.4409443 -0.6929937\nLAC QUI PARLE       1.8680900 -0.6929937\nMILLE LACS          1.2995480 -0.6929937\nPOPE                1.4116740 -0.6929937\nROCK                1.4170797 -0.6929937\nSTEVENS             1.5520593 -0.6929937\nYELLOW MEDICINE     1.3862294 -0.6929937\nBECKER              1.4792143 -0.6929937\nBIG STONE           1.4801817 -0.6929937\nDODGE               1.5840333 -0.6929937\nISANTI              1.3149878 -0.6929937\nKITTSON             1.5015319 -0.6929937\nNOBLES              1.6300755 -0.6929937\nNORMAN              1.3820836 -0.6929937\nPENNINGTON          1.3246513 -0.6929937\nRENVILLE            1.5338618 -0.6929937\nTODD                1.5530274 -0.6929937\nWATONWAN            1.9060483 -0.6929937\nAITKIN              1.1915003 -0.6929937\nBENTON              1.4461503 -0.6929937\nBROWN               1.6827736 -0.6929937\nCHIPPEWA            1.5771520 -0.6929937\nCLEARWATER          1.4024982 -0.6929937\nCOTTONWOOD          1.3723633 -0.6929937\nKANABEC             1.3646863 -0.6929937\nKANDIYOHI           1.7197951 -0.6929937\nLAKE OF THE WOODS   1.6303574 -0.6929937\nLINCOLN             1.8260565 -0.6929937\nNICOLLET            1.7641694 -0.6929937\nPIPESTONE           1.6303984 -0.6929937\nPOLK                1.5675867 -0.6929937\nSIBLEY              1.3673371 -0.6929937\nSWIFT               1.2574762 -0.6929937\nTRAVERSE            1.6938491 -0.6929937\nWASECA              1.0944377 -0.6929937\nCASS                1.4322449 -0.6929937\nHUBBARD             1.3467692 -0.6929937\nJACKSON             1.7329563 -0.6929937\nLE SUEUR            1.5979923 -0.6929937\nMEEKER              1.3416955 -0.6929937\nREDWOOD             1.7120493 -0.6929937\nWADENA              1.3708520 -0.6929937\nCARVER              1.5086099 -0.6929937\nCHISAGO             1.2370518 -0.6929937\nFARIBAULT           1.0211902 -0.6929937\nHOUSTON             1.6222663 -0.6929937\nPINE                1.0877738 -0.6929937\nBELTRAMI            1.5045012 -0.6929937\nKOOCHICHING         1.0870316 -0.6929937\nMARTIN              1.2199767 -0.6929937\nWABASHA             1.6642923 -0.6929937\nLYON                1.7636308 -0.6929937\nOTTER TAIL          1.5494611 -0.6929937\nSHERBURNE           1.2380854 -0.6929937\nDOUGLAS             1.6311136 -0.6929937\nFREEBORN            1.8605721 -0.6929937\nLAKE                0.7928241 -0.6929937\nMARSHALL            1.5404841 -0.6929937\nMORRISON            1.2623707 -0.6929937\nCARLTON             1.1600746 -0.6929937\nSTEELE              1.5389227 -0.6929937\nITASCA              1.0999775 -0.6929937\nRICE                1.7205672 -0.6929937\nCROW WING           1.2209415 -0.6929937\nMCLEOD              1.3375197 -0.6929937\nMOWER               1.6294468 -0.6929937\nSCOTT               1.6981946 -0.6929937\nWINONA              1.5716875 -0.6929937\nWRIGHT              1.5906331 -0.6929937\nBLUE EARTH          1.8581255 -0.6929937\nCLAY                1.8380232 -0.6929937\nGOODHUE             1.8135585 -0.6929937\nROSEAU              1.5982671 -0.6929937\nOLMSTED             1.3328317 -0.6929937\nSTEARNS             1.4829168 -0.6929937\nRAMSEY              1.1995431 -0.6929937\nWASHINGTON          1.3404792 -0.6929937\nANOKA               0.9276468 -0.6929937\nDAKOTA              1.3462611 -0.6929937\nHENNEPIN            1.3626875 -0.6929937\nST LOUIS            0.8899487 -0.6929937\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nEfectos fijos y efectos aleatorios\n\nfixef (M1)\n\n(Intercept)       floor \n  1.4615979  -0.6929937 \n\nranef (M1)\n\n$county_name\n                  (Intercept)\nMAHNOMEN          -0.01597291\nMURRAY             0.16376021\nWILKIN             0.12198054\nCOOK              -0.21829873\nFILLMORE          -0.02065353\nLAC QUI PARLE      0.40649210\nMILLE LACS        -0.16204985\nPOPE              -0.04992386\nROCK              -0.04451817\nSTEVENS            0.09046142\nYELLOW MEDICINE   -0.07536847\nBECKER             0.01761646\nBIG STONE          0.01858386\nDODGE              0.12243538\nISANTI            -0.14661004\nKITTSON            0.03993398\nNOBLES             0.16847766\nNORMAN            -0.07951425\nPENNINGTON        -0.13694658\nRENVILLE           0.07226396\nTODD               0.09142954\nWATONWAN           0.44445038\nAITKIN            -0.27009754\nBENTON            -0.01544759\nBROWN              0.22117574\nCHIPPEWA           0.11555414\nCLEARWATER        -0.05909971\nCOTTONWOOD        -0.08923457\nKANABEC           -0.09691161\nKANDIYOHI          0.25819719\nLAKE OF THE WOODS  0.16875955\nLINCOLN            0.36445858\nNICOLLET           0.30257150\nPIPESTONE          0.16880053\nPOLK               0.10598885\nSIBLEY            -0.09426078\nSWIFT             -0.20412165\nTRAVERSE           0.23225117\nWASECA            -0.36716018\nCASS              -0.02935303\nHUBBARD           -0.11482868\nJACKSON            0.27135840\nLE SUEUR           0.13639437\nMEEKER            -0.11990237\nREDWOOD            0.25045139\nWADENA            -0.09074584\nCARVER             0.04701207\nCHISAGO           -0.22454609\nFARIBAULT         -0.44040766\nHOUSTON            0.16066847\nPINE              -0.37382404\nBELTRAMI           0.04290332\nKOOCHICHING       -0.37456633\nMARTIN            -0.24162116\nWABASHA            0.20269438\nLYON               0.30203295\nOTTER TAIL         0.08786325\nSHERBURNE         -0.22351246\nDOUGLAS            0.16951573\nFREEBORN           0.39897423\nLAKE              -0.66877374\nMARSHALL           0.07888624\nMORRISON          -0.19922714\nCARLTON           -0.30152324\nSTEELE             0.07732481\nITASCA            -0.36162041\nRICE               0.25896936\nCROW WING         -0.24065636\nMCLEOD            -0.12407820\nMOWER              0.16784894\nSCOTT              0.23659675\nWINONA             0.11008962\nWRIGHT             0.12903525\nBLUE EARTH         0.39652763\nCLAY               0.37642531\nGOODHUE            0.35196061\nROSEAU             0.13666919\nOLMSTED           -0.12876623\nSTEARNS            0.02131895\nRAMSEY            -0.26205481\nWASHINGTON        -0.12111868\nANOKA             -0.53395109\nDAKOTA            -0.11533677\nHENNEPIN          -0.09891043\nST LOUIS          -0.57164916\n\nwith conditional variances for \"county_name\" \n\n\nIncertidumbres de los coeficientes estimados\n\nse.fixef (M1)\n\n(Intercept)       floor \n 0.05157623  0.07043081 \n\nse.ranef (M1)\n\n$county_name\n                  (Intercept)\nMAHNOMEN           0.30104585\nMURRAY             0.30104585\nWILKIN             0.30104585\nCOOK               0.27966565\nFILLMORE           0.27966565\nLAC QUI PARLE      0.27966565\nMILLE LACS         0.27966565\nPOPE               0.27966565\nROCK               0.27966565\nSTEVENS            0.27966565\nYELLOW MEDICINE    0.27966565\nBECKER             0.26227671\nBIG STONE          0.26227671\nDODGE              0.26227671\nISANTI             0.26227671\nKITTSON            0.26227671\nNOBLES             0.26227671\nNORMAN             0.26227671\nPENNINGTON         0.26227671\nRENVILLE           0.26227671\nTODD               0.26227671\nWATONWAN           0.26227671\nAITKIN             0.24777409\nBENTON             0.24777409\nBROWN              0.24777409\nCHIPPEWA           0.24777409\nCLEARWATER         0.24777409\nCOTTONWOOD         0.24777409\nKANABEC            0.24777409\nKANDIYOHI          0.24777409\nLAKE OF THE WOODS  0.24777409\nLINCOLN            0.24777409\nNICOLLET           0.24777409\nPIPESTONE          0.24777409\nPOLK               0.24777409\nSIBLEY             0.24777409\nSWIFT              0.24777409\nTRAVERSE           0.24777409\nWASECA             0.24777409\nCASS               0.23543858\nHUBBARD            0.23543858\nJACKSON            0.23543858\nLE SUEUR           0.23543858\nMEEKER             0.23543858\nREDWOOD            0.23543858\nWADENA             0.23543858\nCARVER             0.22477918\nCHISAGO            0.22477918\nFARIBAULT          0.22477918\nHOUSTON            0.22477918\nPINE               0.22477918\nBELTRAMI           0.21544775\nKOOCHICHING        0.21544775\nMARTIN             0.21544775\nWABASHA            0.21544775\nLYON               0.20718964\nOTTER TAIL         0.20718964\nSHERBURNE          0.20718964\nDOUGLAS            0.19981371\nFREEBORN           0.19981371\nLAKE               0.19981371\nMARSHALL           0.19981371\nMORRISON           0.19981371\nCARLTON            0.19317329\nSTEELE             0.19317329\nITASCA             0.18715376\nRICE               0.18715376\nCROW WING          0.18166402\nMCLEOD             0.17663066\nMOWER              0.17663066\nSCOTT              0.17663066\nWINONA             0.17663066\nWRIGHT             0.17663066\nBLUE EARTH         0.17199375\nCLAY               0.17199375\nGOODHUE            0.17199375\nROSEAU             0.17199375\nOLMSTED            0.14203531\nSTEARNS            0.13726758\nRAMSEY             0.12371837\nWASHINGTON         0.10549434\nANOKA              0.09981833\nDAKOTA             0.09142752\nHENNEPIN           0.07194472\nST LOUIS           0.06860507\n\nse_random &lt;- se.ranef (M1)\n\nIncertidumbre de los coeficientes por condado vs tamaño de muestra:\n\ntabla_se_random &lt;- data.frame(county_name = rownames(se_random$county),\n                              se = as.numeric(se_random$county))\n\n\ntabla_se &lt;- radon_data %&gt;% group_by(county_name) %&gt;% summarise(n = n()) %&gt;% \n  left_join(tabla_se_random, by = \"county_name\") \n\n  ggplot(mapping = aes(x = n, y = se),data = tabla_se) +\n  geom_point()\n\n\n\n\n\n\n\n\nIntervalos de confianza para los coeficientes fijos:\n\nfixef(M1)[1]+c(-1,1)*qnorm(0.975)*se.fixef(M1)[1]\n\n[1] 1.360510 1.562685\n\nfixef(M1)[2]+c(-1,1)*qnorm(0.975)*se.fixef(M1)[2]\n\n[1] -0.8310356 -0.5549519\n\n\nIntervalos de confianza para los coeficientes por condado. Intercepto total:\n\ncoef(M1)$county[26,1]+c(-1,1)*qnorm(0.975)*se.ranef(M1)$county[26]\n\n[1] 1.091524 2.062780\n\n\nSolo efecto aleatorio sobre intercepto fijo\n\nas.matrix(ranef(M1)$county)[26]+c(-1,1)*qnorm(0.975)*se.ranef(M1)$county[26]\n\n[1] -0.3700742  0.6011824",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelos Lineales Multinivel básicos</span>"
    ]
  },
  {
    "objectID": "MLMbasicos.html#modelo-multinivel-con-predictor-por-grupo",
    "href": "MLMbasicos.html#modelo-multinivel-con-predictor-por-grupo",
    "title": "2  Modelos Lineales Multinivel básicos",
    "section": "2.3 Modelo multinivel con predictor por grupo",
    "text": "2.3 Modelo multinivel con predictor por grupo\nPreparación de datos:\n\ncty &lt;- read_delim('./data/ARM_Data/radon/cty.dat',trim_ws = TRUE)\n\nRows: 3194 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): stfips, ctfips, st, cty\ndbl (3): lon, lat, Uppm\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nradon_data &lt;- radon_data %&gt;% mutate(fips = stfips * 1000 + cntyfips)\n\n# Obtener el FIPS a nivel de EE.UU.\nusa.fips &lt;- 1000 * as.numeric(cty$stfips) + as.numeric(cty$ctfips)\n\n# Encontrar las filas correspondientes a los condados únicos en Minnesota\nusa.rows &lt;- match(unique(radon_data$fips), usa.fips)\n\nuranium &lt;- cty[usa.rows, \"Uppm\"]\nu &lt;- log(uranium)\n\ncounty_u_data &lt;- data.frame(\n  county_name = unique(radon_data$county_name),\n  u = u\n)\n\nradon_data &lt;- radon_data %&gt;%\n  left_join(county_u_data, by = \"county_name\") %&gt;% \n  rename(u = Uppm)\n\nModelo con intercepto variable por condado y piso y Uranio\n\nM2 &lt;- lmer (log_radon ~ floor + u + (1 | county_name),data = radon_data)\ndisplay (M2)\n\nlmer(formula = log_radon ~ floor + u + (1 | county_name), data = radon_data)\n            coef.est coef.se\n(Intercept)  1.47     0.04  \nfloor       -0.67     0.07  \nu            0.72     0.09  \n\nError terms:\n Groups      Name        Std.Dev.\n county_name (Intercept) 0.16    \n Residual                0.76    \n---\nnumber of obs: 919, groups: county_name, 85\nAIC = 2144.2, DIC = 2111.4\ndeviance = 2122.8 \n\n\nResultados:\n\n#coef(M2)\n#fixef(M2)\n#ranef(M2)\n\nRelación coeficientes aleatorios por condado vs uranio:\n\n# Summarize the data\nu &lt;- radon_data %&gt;%\n  group_by(county_name) %&gt;%\n  summarise(u = mean(u))\n\n# Calculate model coefficients\na.hat.M2 &lt;- fixef(M2)[1] + fixef(M2)[3] * u$u + ranef(M2)$county\nb.hat.M2 &lt;- fixef(M2)[2]\n\n# Standard error (assuming se.coef function calculates this)\na.se.M2 &lt;- se.coef(M2)$county\n\n# Prepare data for plotting\nplot_data &lt;- data.frame(\n  u = u$u,\n  a_hat = a.hat.M2,\n  se = a.se.M2\n) \ncolnames(plot_data) &lt;- c(\"u\", \"a_hat\", \"se\")\n\n# Create the plot\nggplot(plot_data, aes(x = u, y = a_hat)) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, color = \"black\") +\n  labs(x = \"County-level uranium measure\", y = \"Estimated regression intercept\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(-1, 1, 0.5)) +\n  scale_y_continuous(limits = c(0.5, 2.0)) +\n  theme(\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  )",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelos Lineales Multinivel básicos</span>"
    ]
  },
  {
    "objectID": "MLMbasicos.html#predicción-de-un-modelo-multinivel",
    "href": "MLMbasicos.html#predicción-de-un-modelo-multinivel",
    "title": "2  Modelos Lineales Multinivel básicos",
    "section": "2.4 Predicción de un modelo multinivel",
    "text": "2.4 Predicción de un modelo multinivel\n\nn.sims &lt;- 1000\n\nx.tilde &lt;- 1\nsigma.y.hat &lt;- sigma.hat(M2)$sigma$data\ncoef.hat &lt;- as.matrix(coef(M2)$county)[26, ]\nu_value &lt;- county_u_data[26,2]\n\ny.tilde &lt;- rnorm(1, sum(coef.hat * c(1, x.tilde, u_value)), sigma.y.hat)\ny.tilde &lt;- rnorm(n.sims, sum(coef.hat * c(1, x.tilde, u_value)), sigma.y.hat)\n\nmean_y_tilde &lt;- mean(y.tilde)\nquantiles_y_tilde &lt;- quantile(y.tilde, probs = c(0.25, 0.5, 0.75))\n\nunlogged &lt;- exp(y.tilde)\nmean_unlogged &lt;- mean(unlogged)\nquantiles_unlogged &lt;- quantile(unlogged, probs = c(0.25, 0.5, 0.75))\n\nu.tilde &lt;- mean(county_u_data[,2])\ng.0.hat &lt;- fixef(M2)[\"(Intercept)\"]\ng.1.hat &lt;- fixef(M2)[\"u\"]\nsigma.a.hat &lt;- sigma.hat(M2)$sigma$county\nb.hat &lt;- b.hat.M2\n\na.tilde &lt;- rnorm(n.sims, g.0.hat + g.1.hat * u.tilde, sigma.a.hat)\ny.tilde &lt;- rnorm(n.sims, a.tilde + b.hat * x.tilde, sigma.y.hat)\n\nquantiles_y_tilde &lt;- quantile(y.tilde, probs = c(0.25, 0.5, 0.75),na.rm = T)\nexp_quantiles_y_tilde &lt;- exp(quantiles_y_tilde)\n\n# Nonlinear predictions\ny.tilde.basement &lt;- rnorm(n.sims, a.hat.M2[26, ], sigma.y.hat)\ny.tilde.nobasement &lt;- rnorm(n.sims, a.hat.M2[26, ] + b.hat.M2, sigma.y.hat)\ny.tilde &lt;- 0.9 * y.tilde.basement + 0.1 * y.tilde.nobasement\n\nmean_radon_basement &lt;- mean(exp(y.tilde.basement))\nmean_radon_nobasement &lt;- mean(exp(y.tilde.nobasement))\nmean_radon &lt;- 0.9 * mean_radon_basement + 0.1 * mean_radon_basement\n\nprint(mean_radon_basement)\n\n[1] 6.964798\n\nprint(mean_radon_nobasement)\n\n[1] 3.693868\n\nprint(mean_radon)\n\n[1] 6.964798",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modelos Lineales Multinivel básicos</span>"
    ]
  },
  {
    "objectID": "MLMavanzados.html",
    "href": "MLMavanzados.html",
    "title": "3  Modelos Lineales Multinivel avanzados",
    "section": "",
    "text": "3.1 Modelos multinivel no-anidados",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos Lineales Multinivel avanzados</span>"
    ]
  },
  {
    "objectID": "MLMavanzados.html#modelos-multinivel-no-anidados",
    "href": "MLMavanzados.html#modelos-multinivel-no-anidados",
    "title": "3  Modelos Lineales Multinivel avanzados",
    "section": "",
    "text": "3.1.1 Ejemplo 1: Pilotos\nCarga de paquetes necesarios:\n\nlibrary(tidyverse)\nlibrary(arm)\nlibrary(readr)  \n\nCarga de datos:\n\npilots &lt;- read_delim('./data/ARM_Data/pilots/pilots.dat')\n\npilots &lt;- pilots %&gt;% mutate(group = as.factor(group), \n                            scenario = as.factor(scenario),\n                            recovered = ifelse(is.na(recovered),NA,as.numeric(recovered)))\n\nDepuración de datos:\n\nresult &lt;- pilots %&gt;%\n  group_by(group, scenario) %&gt;%\n  summarize(\n    successes = sum(recovered == 1, na.rm = TRUE),\n    failures = sum(recovered == 0, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    y = successes / (successes + failures)\n  )\n\nresult &lt;- result %&gt;%\n  distinct(group, scenario, .keep_all = TRUE)\n\nCambio a formato ancho y ordenamiento de datos:\n\ny_mat &lt;- result %&gt;%\n  pivot_wider(names_from = group, values_from = y, values_fill = 0, id_cols = scenario) %&gt;%\n  column_to_rownames(\"scenario\")\n\n\nsort_group &lt;- order(apply(y_mat, 2, mean, na.rm = TRUE))\nsort_scenario &lt;- order(apply(y_mat, 1, mean, na.rm = TRUE))\n\n\ny_mat_new &lt;- y_mat[sort_scenario, sort_group]\n\nresult &lt;- result %&gt;%\n  mutate(\n    group_id_new = factor(group, levels = colnames(y_mat)[sort_group]),\n    scenario_id_new = factor(scenario, levels = rownames(y_mat)[sort_scenario])\n  )\n\nHeatmap de tasa de exito por grupo y escenario:\n\nggplot(result, aes(x = group_id_new, y = scenario_id_new, fill = y)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient(low = \"blue\", high = \"red\", name = \"Success Rate\") +\n  labs(\n    title = \"Success Rate by Group and Scenario\",\n    x = \"Group\",\n    y = \"Scenario\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.text.y = element_text(angle = 45, vjust = 1)\n  )\n\n\n\n\n\n\n\n\nAjuste de modelo multinivel usando grupos y escenarios:\n\nM1 &lt;- lmer (y ~ 1 + (1 | group) + (1 | scenario),data = result)\n\nboundary (singular) fit: see help('isSingular')\n\ndisplay (M1)\n\nlmer(formula = y ~ 1 + (1 | group) + (1 | scenario), data = result)\ncoef.est  coef.se \n    0.44     0.12 \n\nError terms:\n Groups   Name        Std.Dev.\n scenario (Intercept) 0.32    \n group    (Intercept) 0.00    \n Residual             0.22    \n---\nnumber of obs: 40, groups: scenario, 8; group, 5\nAIC = 20.3, DIC = 7.3\ndeviance = 9.8 \n\n\nAjuste con paquete nlme:\n\nM1_nlme &lt;- nlme::lme(y ~ 1, random = ~ 1 | group / scenario, data = result)\nSS &lt;- summary(M1_nlme)\nSS\n\nLinear mixed-effects model fit by REML\n  Data: result \n       AIC      BIC    logLik\n  45.53959 52.19384 -18.76979\n\nRandom effects:\n Formula: ~1 | group\n         (Intercept)\nStdDev: 7.146626e-06\n\n Formula: ~1 | scenario %in% group\n        (Intercept)    Residual\nStdDev:   0.3734507 0.001986792\n\nFixed effects:  y ~ 1 \n                Value  Std.Error DF  t-value p-value\n(Intercept) 0.4418155 0.05904858 35 7.482237       0\n\nStandardized Within-Group Residuals:\n         Min           Q1          Med           Q3          Max \n-0.006293823 -0.004513152 -0.001248589  0.005916490  0.007951542 \n\nNumber of Observations: 40\nNumber of Groups: \n              group scenario %in% group \n                  5                  40 \n\n\n\n\n3.1.2 Ejemplo 2: Ganancias vs altura\nCarga de datos:\n\nlibrary(haven)\nheights &lt;- read_dta(\"data/ARM_Data/earnings/heights.dta\")\n\nDepuracion de datos\n\nheights &lt;- heights %&gt;%\n  mutate(\n    age = 90 - yearbn,\n    age = ifelse(age &lt; 18, NA, age),\n    age_category = case_when(\n      age &lt; 35 ~ 1,\n      age &lt; 50 ~ 2,\n      TRUE ~ 3\n    ),\n    eth = case_when(\n      race == 2 ~ 1,\n      hisp == 1 ~ 2,\n      race == 1 ~ 3,\n      TRUE ~ 4\n    ),\n    male = 2 - sex\n  )\n\nheights_clean &lt;- heights %&gt;%\n  filter(!is.na(earn) & !is.na(height) & !is.na(sex) & earn &gt; 0 & yearbn &gt; 25) %&gt;%\n  dplyr::select(earn, height, sex, race, hisp, ed, age, age_category, eth, male)\n\nAlgunas correcciones posteriores y variables adicionales:\n\nheights_clean &lt;- heights_clean %&gt;%\n  mutate(height_jitter_add = runif(n(), -.2, .2))\n\nheights_clean &lt;- heights_clean %&gt;%\n  mutate(\n    y = log(earn),\n    x = height\n  )\n\ny &lt;- heights_clean$y\nx &lt;- heights_clean$x\nage &lt;- heights_clean$age_category\n\nn &lt;- nrow(heights_clean)\nn_age &lt;- 3\nn_eth &lt;- 4\n\nAjuste log-ingresos vs altura por etnia (Modelo anidado):\n\nM1 &lt;- lmer (y ~ x + (1 + x | eth),data = heights_clean)\n\nboundary (singular) fit: see help('isSingular')\n\ndisplay (M1)\n\nlmer(formula = y ~ x + (1 + x | eth), data = heights_clean)\n            coef.est coef.se\n(Intercept) 7.27     1.10   \nx           0.04     0.02   \n\nError terms:\n Groups   Name        Std.Dev. Corr  \n eth      (Intercept) 1.55           \n          x           0.02     -1.00 \n Residual             0.90           \n---\nnumber of obs: 1062, groups: eth, 4\nAIC = 2823.3, DIC = 2788.6\ndeviance = 2800.0 \n\n\nAjuste del modelo mixto usando nlme con interceptos y pendientes aleatorios:\n\nM1_nlme &lt;- nlme::lme(\n  fixed = y ~ x, \n  random = ~ x | eth, \n  data = heights_clean\n)\nsummary(M1_nlme)\n\nLinear mixed-effects model fit by REML\n  Data: heights_clean \n       AIC      BIC    logLik\n  2823.787 2853.583 -1405.893\n\nRandom effects:\n Formula: ~x | eth\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 1.77599145 (Intr)\nx           0.02742268 -1    \nResidual    0.90300309       \n\nFixed effects:  y ~ x \n               Value Std.Error   DF  t-value p-value\n(Intercept) 7.380834 1.2082771 1057 6.108560  0.0000\nx           0.034270 0.0185317 1057 1.849284  0.0647\n Correlation: \n  (Intr)\nx -0.999\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-4.9284272 -0.4286750  0.1752600  0.6456474  2.6066249 \n\nNumber of Observations: 1062\nNumber of Groups: 4 \n\n\nCorrección para reducir correlación:\n\nx.centered &lt;- x - mean(x)\n\nM2 &lt;- lmer (y ~ x.centered + (1 + x.centered | eth),data = heights_clean)\n\nboundary (singular) fit: see help('isSingular')\n\ndisplay (M2)\n\nlmer(formula = y ~ x.centered + (1 + x.centered | eth), data = heights_clean)\n            coef.est coef.se\n(Intercept) 9.68     0.05   \nx.centered  0.03     0.02   \n\nError terms:\n Groups   Name        Std.Dev. Corr \n eth      (Intercept) 0.07          \n          x.centered  0.03     1.00 \n Residual             0.90          \n---\nnumber of obs: 1062, groups: eth, 4\nAIC = 2823.3, DIC = 2788.6\ndeviance = 2800.0 \n\n\nAjuste de un modelo mixto con nlme (comentado porque da error):\n\n#M2_nlme &lt;- nlme::lme(\n#  fixed = y ~ x.centered, \n#  random = ~ x.centered | eth, \n#  data = heights_clean\n#)\n\nInclusión de la categoría de edad:\n\nM3 &lt;- lmer (y ~ x.centered + (1 + x.centered | eth) + (1 + x.centered | age) +  \n              (1 + x.centered | eth:age),data = heights_clean)\n\nboundary (singular) fit: see help('isSingular')\n\ndisplay (M3)\n\nlmer(formula = y ~ x.centered + (1 + x.centered | eth) + (1 + \n    x.centered | age) + (1 + x.centered | eth:age), data = heights_clean)\n            coef.est coef.se\n(Intercept) 9.69     0.07   \nx.centered  0.05     0.02   \n\nError terms:\n Groups   Name        Std.Dev. Corr  \n eth:age  (Intercept) 0.00           \n          x.centered  0.00     -0.90 \n age      (Intercept) 0.42           \n          x.centered  0.02     0.76  \n eth      (Intercept) 0.04           \n          x.centered  0.02     1.00  \n Residual             0.81           \n---\nnumber of obs: 1059, groups: eth:age, 136; age, 47; eth, 4\nAIC = 2697.1, DIC = 2652.6\ndeviance = 2662.9 \n\n\nAjuste del modelo con nlme (con error):\n\n#M3_nlme &lt;- nlme::lme(\n#  fixed = y ~ x.centered, \n#  random = ~ x.centered | eth + age + eth:age, \n#  data = heights_clean\n#)\n\nModelo reducido sin interacción (también da error):\n\n#M4_nlme &lt;- nlme::lme(\n#  fixed = y ~ x, \n#  random = ~ x | eth + age + eth:age, \n#  data = heights_clean\n#)\n\n\n\n3.1.3 Tarea 1\nCarga de datos\n\nlibrary(ggplot2)\nlibrary(readr)\n\ncd4_data &lt;- read_csv(\"data/ARM_Data/cd4/allvar.csv\")\n\nDepuración de datos:\n\ncd4_data$VDATE &lt;- as.Date(cd4_data$VDATE, format=\"%m/%d/%Y\")\ncd4_data_filtered &lt;- na.omit(cd4_data[, c(\"newpid\", \"visage\", \"CD4PCT\", \"baseage\", \"treatmnt\")])\ncd4_data_filtered$time_since_baseage &lt;- cd4_data_filtered$visage - cd4_data_filtered$baseage\n\ncd4_data_filtered$sqrt_CD4PCT &lt;- sqrt(cd4_data_filtered$CD4PCT)\n\nGráfico del porcentaje transformado de CD4 como función del tiempo:\n\nggplot(cd4_data_filtered, aes(x=time_since_baseage, y=sqrt_CD4PCT, group=newpid, color=factor(newpid))) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Time since Base Age (Years)\", y = \"Square Root of CD4 Percentage\", \n       title = \"Square Root of CD4 Percentage over Time for Each Child\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nAjuste de un modelo lineal para cada niño, usando solamente aquellos niños con más de dos visitas:\n\ncd4_data_filtered &lt;- cd4_data_filtered %&gt;%\n  group_by(newpid) %&gt;%\n  filter(n() &gt; 2) %&gt;%\n  ungroup()\n\nlinear_models &lt;- cd4_data_filtered %&gt;%\n  group_by(newpid) %&gt;%\n  do(model = lm(sqrt_CD4PCT ~ time_since_baseage, data = .))\n\ncd4_fits &lt;- linear_models %&gt;%\n  rowwise() %&gt;%\n  do(data.frame(newpid = .$newpid, time_since_baseage = cd4_data_filtered$time_since_baseage, \n                pred = predict(.$model, newdata = cd4_data_filtered)))\n\ncd4_fits &lt;- left_join(cd4_fits, cd4_data_filtered, by = c(\"newpid\", \"time_since_baseage\"))\n\nGráfico de los ajustes lineales para una muestra de 30 niños:\n\nset.seed(123)\nsampled_children &lt;- cd4_data_filtered %&gt;%\n  distinct(newpid) %&gt;%\n  sample_n(30) %&gt;%\n  pull(newpid)\n\ncd4_data_sampled &lt;- cd4_fits %&gt;%\n  filter(newpid %in% sampled_children)\n\nggplot(cd4_data_sampled, aes(x=time_since_baseage, y=sqrt_CD4PCT)) +\n  geom_point(aes(y=sqrt_CD4PCT), alpha=0.6) +  # Original data points\n  geom_line(aes(y=pred), size=1, color=\"blue\") +  # Linear fits\n  facet_wrap(~newpid, scales = \"free_y\") +    # Facet by patient ID\n  labs(x = \"Tiempo desde edad base\", y = \"Raíz de CD4 (%)\", \n       title = \"\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAjuste de un modelo con interceptos y pendientes como función del tratamiento y la edad base, a través de un procedimiento de dos pasos. Paso 1:\n\nchild_models &lt;- cd4_data_filtered %&gt;%\n  group_by(newpid) %&gt;%\n  summarize(\n    intercept = coef(lm(sqrt(CD4PCT) ~ time_since_baseage))[1],  # Intercept\n    slope = coef(lm(sqrt(CD4PCT) ~ time_since_baseage))[2]       # Slope\n  )\n\nMerge de los datos de los modelos con los datos originales:\n\nchild_models &lt;- child_models %&gt;%\n  left_join(cd4_data_filtered %&gt;% distinct(newpid, treatmnt, baseage), by = \"newpid\")\n\nPaso 2:\n\nintercept_model &lt;- lm(intercept ~ treatmnt + baseage, data = child_models)\nslope_model &lt;- lm(slope ~ treatmnt + baseage, data = child_models)\n\nsummary(intercept_model)\n\n\nCall:\nlm(formula = intercept ~ treatmnt + baseage, data = child_models)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9042 -0.6859  0.1141  1.0212  2.7661 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.76860    0.35152   13.57   &lt;2e-16 ***\ntreatmnt     0.37018    0.20007    1.85   0.0659 .  \nbaseage     -0.11327    0.04442   -2.55   0.0116 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.362 on 184 degrees of freedom\nMultiple R-squared:  0.0532,    Adjusted R-squared:  0.0429 \nF-statistic: 5.169 on 2 and 184 DF,  p-value: 0.006546\n\nsummary(slope_model)\n\n\nCall:\nlm(formula = slope ~ treatmnt + baseage, data = child_models)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9019 -0.4392  0.0026  0.4896  6.0402 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.40791    0.29048  -1.404    0.162\ntreatmnt     0.06036    0.16533   0.365    0.715\nbaseage      0.00881    0.03671   0.240    0.811\n\nResidual standard error: 1.126 on 184 degrees of freedom\nMultiple R-squared:  0.0009983, Adjusted R-squared:  -0.00986 \nF-statistic: 0.09193 on 2 and 184 DF,  p-value: 0.9122\n\n\nAjuste de un modelo mixto con intercepto aleatorio por niño:\n\ncd4_model &lt;- lmer(sqrt_CD4PCT ~ time_since_baseage + (1 | newpid), data = cd4_data_filtered)\n\nsummary(cd4_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: sqrt_CD4PCT ~ time_since_baseage + (1 | newpid)\n   Data: cd4_data_filtered\n\nREML criterion at convergence: 2762.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.7884 -0.4721  0.0026  0.4547  5.0256 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newpid   (Intercept) 1.7498   1.3228  \n Residual             0.5865   0.7658  \nNumber of obs: 972, groups:  newpid, 187\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)         4.93400    0.10534  46.839\ntime_since_baseage -0.38491    0.05439  -7.077\n\nCorrelation of Fixed Effects:\n            (Intr)\ntim_snc_bsg -0.313\n\n\nAjuste del modelo extendido con covariables por niño:\n\ncd4_model_extended &lt;- lmer(sqrt_CD4PCT ~ time_since_baseage + treatmnt + baseage + \n                           (1 | newpid), data = cd4_data_filtered)\n\nsummary(cd4_model_extended)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: sqrt_CD4PCT ~ time_since_baseage + treatmnt + baseage + (1 |  \n    newpid)\n   Data: cd4_data_filtered\n\nREML criterion at convergence: 2757.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.8101 -0.4699  0.0084  0.4432  5.0437 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newpid   (Intercept) 1.6680   1.2915  \n Residual             0.5864   0.7658  \nNumber of obs: 972, groups:  newpid, 187\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)         4.66797    0.34667  13.465\ntime_since_baseage -0.38378    0.05438  -7.058\ntreatmnt            0.42721    0.19649   2.174\nbaseage            -0.10143    0.04361  -2.326\n\nCorrelation of Fixed Effects:\n            (Intr) tm_sn_ trtmnt\ntim_snc_bsg -0.090              \ntreatmnt    -0.846 -0.001       \nbaseage     -0.478 -0.009  0.042\n\n\nExtrae la edad del niño en la última visita:\n\nlast_time &lt;- cd4_data_filtered %&gt;%\n  group_by(newpid) %&gt;%\n  summarize(last_visage = max(visage), baseage = first(baseage), treatmnt = first(treatmnt))\n\nCalcula la edad en la siguiente visita, asumiendo que esta se realiza un año después de la última visita:\n\nnext_time_data &lt;- last_time %&gt;%\n  mutate(\n    next_visage = last_visage + 1,  # Hypothetical next time point: 1 year later\n    time_since_baseage = next_visage - baseage  # Recalculate time since base age\n  )\n\nPredicción del CD4 un año después, usando predict (sin simular)\n\nnext_time_data$sqrt_predicted_CD4PCT &lt;- predict(cd4_model_extended, newdata = next_time_data, re.form = ~(1 | newpid))\n\nnext_time_data$predicted_CD4PCT &lt;- (next_time_data$sqrt_predicted_CD4PCT)^2\n\nPredicción de CD4 para un niño nuevo de 4 años, en incrementos de 1 año hasta los 10 años, usando predict (sin simular)\n\nnew_child_data &lt;- data.frame(\n  newpid = \"new_child\",  # Placeholder for new child ID\n  baseage = 4,           # Baseline age is 4 years\n  next_visage = seq(4, 10, by = 1),  # Time points: 4, 5, 6, ..., 10 years\n  treatmnt = 1           # Assume the child is receiving treatment (can change to 0 if no treatment)\n)\n\nnew_child_data$time_since_baseage = new_child_data$next_visage - new_child_data$baseage\n\nnew_child_data$sqrt_predicted_CD4PCT &lt;- predict(cd4_model_extended, newdata = new_child_data, re.form = NA)\n\nnew_child_data$predicted_CD4PCT &lt;- (new_child_data$sqrt_predicted_CD4PCT)^2\n\nnew_child_data %&gt;% dplyr::select(next_visage, predicted_CD4PCT)\n\n  next_visage predicted_CD4PCT\n1           4        21.991143\n2           5        18.538967\n3           6        15.381368\n4           7        12.518345\n5           8         9.949898\n6           9         7.676027\n7          10         5.696733",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos Lineales Multinivel avanzados</span>"
    ]
  },
  {
    "objectID": "cap16.html",
    "href": "cap16.html",
    "title": "No-Pooling Regression with No Constant Term",
    "section": "",
    "text": "4.1 Introduction to Multilevel Modeling in R and Bugs\nMultilevel models allow intercepts and slopes to vary across groups. These models are useful when there is hierarchical data (e.g., individuals nested within schools, or measurements within counties).\nR functions like lm() and lmer() can be used for classical and multilevel models, but they rely on point estimates and may have limitations, especially with small sample sizes or complicated models.\nBugs is introduced as a solution for complex multilevel models, enabling full Bayesian inference which accounts for uncertainty in parameter estimates.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>No-Pooling Regression with No Constant Term</span>"
    ]
  },
  {
    "objectID": "cap16.html#introduction-to-multilevel-modeling-in-r-and-bugs",
    "href": "cap16.html#introduction-to-multilevel-modeling-in-r-and-bugs",
    "title": "No-Pooling Regression with No Constant Term",
    "section": "",
    "text": "4.1.1 Key Steps in Multilevel Modeling:\n\nStart with classical regression using lm() or glm() in R.\nAdd varying intercepts/slopes with lmer().\nUse JAGS or STAN to fit fully Bayesian models when needed.\nFor very large models, further programming in R may be required.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>No-Pooling Regression with No Constant Term</span>"
    ]
  },
  {
    "objectID": "cap16.html#bayesian-inference-and-prior-distributions",
    "href": "cap16.html#bayesian-inference-and-prior-distributions",
    "title": "No-Pooling Regression with No Constant Term",
    "section": "4.2 Bayesian inference and prior distributions",
    "text": "4.2 Bayesian inference and prior distributions\nThe main challenge in fitting a multilevel model is estimating both the data-level regression (which includes the coefficients for all group indicators) and the group-level model. The most direct approach is through Bayesian inference, which uses the group-level model as “prior information” to help estimate individual-level coefficients.\nIn Bayesian multilevel models, prior distributions are required for all parameters. These priors typically fall into two categories:\n\nGroup-level models, often represented by normal distributions, where the mean and standard deviation are assigned noninformative priors.\nNoninformative uniform distributions, used when there is little prior information about the parameters.\n\n\n4.2.1 Classical Regression and Generalized Linear Models\nIn classical regression and generalized linear models (GLMs), these models can be seen as special cases of multilevel modeling, where no group-level model is specified, meaning there is no prior information in the Bayesian sense. The key points are:\n\n4.2.1.1 Classical Regression Model\nIn classical regression, the model is represented as:\n\\[\ny_i = X_i \\beta + \\epsilon_i\n\\]\nwith independent errors:\n\\[\n\\epsilon_i \\sim N(0, \\sigma_y^2)\n\\]\nThe classical model assumes no prior structure on the parameters, where:\n\nThe coefficients \\(\\beta\\) have a uniform prior distribution over the entire real number range \\((-\\infty, \\infty)\\).\nThe error variance \\(\\sigma_y\\) has a uniform prior distribution on \\((0, \\infty)\\).\n\n\n\n4.2.1.2 Classical Logistic Regression Model\nIn classical logistic regression, the probability that \\(y_i = 1\\) is given by:\n\\[\n\\text{Pr}(y_i = 1) = \\text{logit}^{-1}(X_i \\beta)\n\\]\nAgain, the prior on the components of \\(\\beta\\) is uniform, meaning there is no assumption or restriction on the parameter values.\n\n\n\n4.2.2 Simplest Varying-Intercept Model\nThe simplest form of multilevel regression is the varying-intercept model, where both individual and group-level errors are normally distributed. The model can be expressed as:\n\\[\ny_i \\sim N(\\alpha_{j[i]} + \\beta x_i, \\sigma_y^2)\n\\]\nand:\n\\[\n\\alpha_j \\sim N(\\mu_\\alpha, \\sigma_\\alpha^2)\n\\]\n\n4.2.2.1 Interpretation:\n\nThe group-specific intercepts \\(\\alpha_j\\) follow a normal distribution, which can be thought of as a prior distribution for the intercepts in a Bayesian framework.\nThe parameters \\(\\mu_\\alpha\\) and \\(\\sigma_\\alpha\\) are hyperparameters that describe the distribution of the group-level intercepts and are estimated from the data.\n\n\n\n4.2.2.2 Prior Distributions in Bayesian Inference:\n\nIn Bayesian inference, all parameters, including hyperparameters \\(\\mu_\\alpha\\), \\(\\sigma_\\alpha\\), \\(\\beta\\), and \\(\\sigma_y\\), must have prior distributions.\nTypically, these priors are set to noninformative uniform distributions when there is little prior knowledge about the parameters.\n\nThe complete prior distribution is written as:\n\\[\np(\\alpha, \\beta, \\mu_\\alpha, \\sigma_y, \\sigma_\\alpha) \\propto \\prod_{j=1}^{J} N(\\alpha_j | \\mu_\\alpha, \\sigma_\\alpha^2)\n\\]\nThis indicates that the intercepts ( _j ) are modeled independently, following normal distributions, and their probability densities are multiplied to form the joint prior density.\n\n\n\n4.2.3 Varying-Intercept, Varying-Slope Model\nA more complex form of multilevel modeling is the varying-intercept, varying-slope model, where both the intercepts and slopes vary across groups. The model is expressed as:\n\\[\ny_i = \\alpha_{j[i]} + \\beta_{j[i]} x_i + \\epsilon_i\n\\]\n\n4.2.3.1 Model Parameters:\n\nThis model has \\(2J\\) parameters, represented by \\(J\\) pairs: \\((\\alpha_j, \\beta_j)\\).\nEach group \\(j\\) has its own intercept \\(\\alpha_j\\) and slope \\(\\beta_j\\), which vary across groups.\n\n\n\n4.2.3.2 Prior Distributions:\n\nThe prior distribution for the pairs \\((\\alpha_j, \\beta_j)\\) is a bivariate normal distribution, allowing for correlation between the intercept and slope within each group.\nThe hyperparameters governing the distribution of \\(\\alpha_j\\) and \\(\\beta_j\\) (such as the means and variances) are typically assigned independent uniform prior distributions when no prior knowledge is available.\n\n\n\n4.2.3.3 Joint Prior Distribution:\nThe joint prior for the intercepts and slopes across all groups can be expressed as a bivariate normal distribution, which captures the relationship between the group-level intercepts and slopes:\n\\[\n(\\alpha_j, \\beta_j) \\sim \\text{Bivariate Normal}(\\mu_{\\alpha}, \\mu_{\\beta}, \\Sigma)\n\\]\nWhere: - \\(\\mu_{\\alpha}\\) and \\(\\mu_{\\beta}\\) are the means of the intercepts and slopes. - \\(\\Sigma\\) is the covariance matrix, which captures the variances of the intercepts and slopes and the correlation between them.\n\n\n\n4.2.4 Multilevel Model with Group-Level Predictors: Exchangeability and Prior Distributions\nIn multilevel models, we typically do not assign models to the coefficients of group-level or individual-level predictors that do not vary by group. In Bayesian terminology, we often assign noninformative uniform prior distributions to these coefficients.\n\n4.2.4.1 Group-Level Regression and Prior Distributions:\nA group-level regression induces different prior distributions on the group-level coefficients. Consider the following simple varying-intercept model with one predictor at the individual level and one at the group level:\n\\[\ny_i = \\alpha_{j[i]} + \\beta x_i + \\epsilon_i, \\ \\epsilon_i \\sim N(0, \\sigma_y^2), \\ \\text{for} \\ i = 1, \\ldots, n\n\\]\n\\[\n\\alpha_j = \\gamma_0 + \\gamma_1 u_j + \\eta_j, \\ \\eta_j \\sim N(0, \\sigma_\\alpha^2), \\ \\text{for} \\ j = 1, \\ldots, J\n\\]\n\n\n4.2.4.2 Model Interpretation:\n\nThe first equation is the data model or likelihood.\nThe second equation is the group-level model or prior model for the intercepts \\(\\alpha_j\\).\nThe intercepts \\(\\alpha_j\\) have different prior distributions based on the group-level predictor \\(u_j\\). The mean of \\(\\alpha_j\\) is \\(\\hat{\\alpha_j} = \\gamma_0 + \\gamma_1 u_j\\), and the standard deviation is \\(\\sigma_\\alpha\\).\n\n\n\n4.2.4.3 Exchangeability of the Group-Level Errors:\nAn equivalent way to think of this model is that the group-level errors \\(\\eta_j\\) are exchangeable:\n\\[\n\\alpha_j = \\gamma_0 + \\gamma_1 u_j + \\eta_j, \\ \\eta_j \\sim N(0, \\sigma_\\alpha^2)\n\\]\nIn this view, the \\(\\alpha_j\\)’s are determined by the group-level predictor \\(u_j\\) and the group-level error \\(\\eta_j\\), with \\(\\eta_j\\) assigned a common prior distribution.\n\n\n4.2.4.4 Key Insights:\n\nThe prior distribution in a multilevel model can be viewed in two ways:\n\nAs a model representing a group-level estimate for each \\(\\alpha_j\\).\nAs a single model representing the distribution of the group-level errors \\(\\eta_j\\).\n\n\nIn practical applications, it is often more efficient to use the first approach when working with group-level predictors in software like Bugs, as this reduces the number of variables and speeds up the computation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>No-Pooling Regression with No Constant Term</span>"
    ]
  },
  {
    "objectID": "cap16.html#fitting-and-understanding-a-varying-intercept-multilevel-model",
    "href": "cap16.html#fitting-and-understanding-a-varying-intercept-multilevel-model",
    "title": "No-Pooling Regression with No Constant Term",
    "section": "4.3 Fitting and Understanding a Varying-Intercept Multilevel Model",
    "text": "4.3 Fitting and Understanding a Varying-Intercept Multilevel Model\n\n4.3.1 Loading Data in R\nWe use radon measurements and floor indicators (basement or first floor) for 919 homes in 85 counties in Minnesota. Since we assume multiplicative effects, we work with the logarithms of radon levels. Any radon measurements recorded as 0.0 are corrected to 0.1 before taking the logarithm.\n\nsrrs2 &lt;- read.table(\"./data/ARM_Data/radon/srrs2.dat\", header=TRUE, sep=\",\")\nmn &lt;- srrs2$state == \"MN\"\nradon &lt;- srrs2$activity[mn]\ny &lt;- log(ifelse(radon == 0, 0.1, radon))\nn &lt;- length(radon)\nx &lt;- srrs2$floor[mn]  # 0 for basement, 1 for first floor\n\nTo account for county-specific effects, we create a county-level indicator for each observation:\n\nsrrs2.fips &lt;- srrs2$stfips * 1000 + srrs2$cntyfips\ncounty.name &lt;- as.vector(srrs2$county[mn])\nuniq.name &lt;- unique(county.name)\nJ &lt;- length(uniq.name)\ncounty &lt;- rep(NA, J)\nfor (i in 1:J) {\n  county[county.name == uniq.name[i]] &lt;- i\n}\n\n\n\n4.3.2 Classical Complete-Pooling Regression in R\nWe start by fitting a classical regression model that ignores county-level differences, treating the entire dataset as if it comes from a single population. This approach is known as complete pooling.\n\n# Complete-pooling model\nlm.pooled &lt;- lm(y ~ x)\n\n# Display results\nsummary(lm.pooled)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6293 -0.5383  0.0342  0.5603  2.5486 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.32674    0.02972  44.640   &lt;2e-16 ***\nx           -0.61339    0.07284  -8.421   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8226 on 917 degrees of freedom\nMultiple R-squared:  0.07178,   Adjusted R-squared:  0.07077 \nF-statistic: 70.91 on 1 and 917 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n4.3.3 Classical No-Pooling Regression in R\nIn the no-pooling model, we include county-specific indicators to allow each county to have its own intercept. This model includes 85 counties but uses 84 indicators since we already have a constant term.\n\n# No-pooling model\nlm.unpooled.0 &lt;- lm(formula = y ~ x + factor(county))\n\n# Display results\nsummary(lm.unpooled.0)\n\n\nCall:\nlm(formula = y ~ x + factor(county))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.14595 -0.45405  0.00065  0.45376  2.65987 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.84054    0.37866   2.220  0.02670 *  \nx                -0.72054    0.07352  -9.800  &lt; 2e-16 ***\nfactor(county)2   0.03428    0.39274   0.087  0.93047    \nfactor(county)3   0.68816    0.57854   1.189  0.23459    \nfactor(county)4   0.71218    0.47470   1.500  0.13392    \nfactor(county)5   0.59203    0.53487   1.107  0.26867    \nfactor(county)6   0.67247    0.57802   1.163  0.24500    \nfactor(county)7   1.17162    0.42892   2.732  0.00644 ** \nfactor(county)8   1.14904    0.53519   2.147  0.03208 *  \nfactor(county)9   0.16250    0.44764   0.363  0.71669    \nfactor(county)10  0.72336    0.48861   1.480  0.13913    \nfactor(county)11  0.56059    0.50775   1.104  0.26988    \nfactor(county)12  0.88971    0.53519   1.662  0.09680 .  \nfactor(county)13  0.19818    0.48861   0.406  0.68514    \nfactor(county)14  1.14784    0.42886   2.677  0.00759 ** \nfactor(county)15  0.49743    0.53519   0.929  0.35292    \nfactor(county)16 -0.17568    0.65534  -0.268  0.78871    \nfactor(county)17  0.43426    0.53613   0.810  0.41818    \nfactor(county)18  0.28101    0.43672   0.643  0.52011    \nfactor(county)19  0.49777    0.39027   1.275  0.20251    \nfactor(county)20  0.95977    0.57802   1.660  0.09720 .  \nfactor(county)21  0.89345    0.45467   1.965  0.04974 *  \nfactor(county)22 -0.20375    0.48831  -0.417  0.67660    \nfactor(county)23  0.55945    0.65534   0.854  0.39353    \nfactor(county)24  1.26108    0.45456   2.774  0.00566 ** \nfactor(county)25  1.11018    0.42892   2.588  0.00981 ** \nfactor(county)26  0.52004    0.38549   1.349  0.17770    \nfactor(county)27  0.93282    0.48831   1.910  0.05644 .  \nfactor(county)28  0.40105    0.50807   0.789  0.43013    \nfactor(county)29  0.21546    0.57802   0.373  0.70942    \nfactor(county)30  0.08522    0.44204   0.193  0.84718    \nfactor(county)31  1.18003    0.50775   2.324  0.02036 *  \nfactor(county)32  0.39575    0.53519   0.739  0.45983    \nfactor(county)33  1.22133    0.53519   2.282  0.02274 *  \nfactor(county)34  0.74990    0.57854   1.296  0.19527    \nfactor(county)35 -0.02134    0.47470  -0.045  0.96415    \nfactor(county)36  2.11842    0.65534   3.233  0.00128 ** \nfactor(county)37 -0.43845    0.45467  -0.964  0.33516    \nfactor(county)38  1.02717    0.53519   1.919  0.05529 .  \nfactor(county)39  0.90752    0.50743   1.788  0.07407 .  \nfactor(county)40  1.47526    0.53487   2.758  0.00594 ** \nfactor(county)41  1.12661    0.46330   2.432  0.01524 *  \nfactor(county)42  0.52044    0.84590   0.615  0.53856    \nfactor(county)43  0.76170    0.45511   1.674  0.09457 .  \nfactor(county)44  0.20045    0.47418   0.423  0.67261    \nfactor(county)45  0.45487    0.43252   1.052  0.29325    \nfactor(county)46  0.37407    0.50775   0.737  0.46150    \nfactor(county)47  0.04339    0.65534   0.066  0.94723    \nfactor(county)48  0.30758    0.45467   0.676  0.49892    \nfactor(county)49  0.86157    0.43256   1.992  0.04672 *  \nfactor(county)50  1.65266    0.84590   1.954  0.05107 .  \nfactor(county)51  1.32450    0.53519   2.475  0.01353 *  \nfactor(county)52  1.08715    0.57802   1.881  0.06034 .  \nfactor(county)53  0.41026    0.57776   0.710  0.47784    \nfactor(county)54  0.46621    0.40987   1.137  0.25567    \nfactor(county)55  0.77745    0.46330   1.678  0.09371 .  \nfactor(county)56  0.26056    0.57854   0.450  0.65256    \nfactor(county)57 -0.07836    0.48831  -0.160  0.87255    \nfactor(county)58  1.02038    0.53487   1.908  0.05677 .  \nfactor(county)59  0.88124    0.53519   1.647  0.10002    \nfactor(county)60  0.43885    0.65534   0.670  0.50327    \nfactor(county)61  0.31819    0.40132   0.793  0.42809    \nfactor(county)62  1.14247    0.50743   2.251  0.02462 *  \nfactor(county)63  0.83016    0.57776   1.437  0.15113    \nfactor(county)64  1.00729    0.44181   2.280  0.02286 *  \nfactor(county)65  0.45858    0.65534   0.700  0.48427    \nfactor(county)66  0.82520    0.42950   1.921  0.05503 .  \nfactor(county)67  0.96258    0.43252   2.226  0.02631 *  \nfactor(county)68  0.24948    0.46357   0.538  0.59060    \nfactor(county)69  0.40191    0.53519   0.751  0.45288    \nfactor(county)70  0.02709    0.38476   0.070  0.94388    \nfactor(county)71  0.65130    0.40740   1.599  0.11027    \nfactor(county)72  0.73936    0.44788   1.651  0.09916 .  \nfactor(county)73  0.95122    0.65534   1.451  0.14702    \nfactor(county)74  0.14650    0.53519   0.274  0.78436    \nfactor(county)75  0.88318    0.57776   1.529  0.12674    \nfactor(county)76  1.16790    0.53487   2.184  0.02928 *  \nfactor(county)77  0.98114    0.47418   2.069  0.03884 *  \nfactor(county)78  0.44515    0.50754   0.877  0.38070    \nfactor(county)79 -0.22566    0.53487  -0.422  0.67321    \nfactor(county)80  0.48898    0.39445   1.240  0.21545    \nfactor(county)81  1.86899    0.57854   3.231  0.00128 ** \nfactor(county)82  1.38947    0.84590   1.643  0.10084    \nfactor(county)83  0.78238    0.43250   1.809  0.07082 .  \nfactor(county)84  0.80481    0.43269   1.860  0.06323 .  \nfactor(county)85  0.34598    0.65534   0.528  0.59768    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7564 on 833 degrees of freedom\nMultiple R-squared:  0.287, Adjusted R-squared:  0.2142 \nF-statistic: 3.945 on 85 and 833 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation:\n\nCounty-specific intercepts represent differences from the reference (County 1).\nFor instance, log radon levels in County 2 are \\(0.03\\) higher than County 1.\nThis model fits better than the complete-pooling model (with a lower residual SD and higher \\(R^2\\)).\nHowever, estimates for individual counties may be uncertain, especially for smaller counties.\n\n\n\n4.3.4 No-Pooling Regression with No Constant Term\nTo fit a model where each county has its own intercept without a constant term, we use the -1 in the formula. This allows each county to have its own intercept, making predictions for individual counties more convenient.\n\n# No-pooling model with no constant term\nlm.unpooled &lt;- lm(formula = y ~ x + factor(county) - 1)\n\n# Display results\nsummary(lm.unpooled)\n\n\nCall:\nlm(formula = y ~ x + factor(county) - 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.14595 -0.45405  0.00065  0.45376  2.65987 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \nx                -0.72054    0.07352  -9.800  &lt; 2e-16 ***\nfactor(county)1   0.84054    0.37866   2.220 0.026701 *  \nfactor(county)2   0.87482    0.10498   8.333 3.23e-16 ***\nfactor(county)3   1.52870    0.43946   3.479 0.000530 ***\nfactor(county)4   1.55272    0.28897   5.373 1.00e-07 ***\nfactor(county)5   1.43257    0.37866   3.783 0.000166 ***\nfactor(county)6   1.51301    0.43672   3.464 0.000558 ***\nfactor(county)7   2.01216    0.20243   9.940  &lt; 2e-16 ***\nfactor(county)8   1.98958    0.37999   5.236 2.08e-07 ***\nfactor(county)9   1.00304    0.23931   4.191 3.07e-05 ***\nfactor(county)10  1.56391    0.31099   5.029 6.04e-07 ***\nfactor(county)11  1.40113    0.33828   4.142 3.80e-05 ***\nfactor(county)12  1.73025    0.37821   4.575 5.49e-06 ***\nfactor(county)13  1.03872    0.30881   3.364 0.000804 ***\nfactor(county)14  1.98838    0.20325   9.783  &lt; 2e-16 ***\nfactor(county)15  1.33797    0.37999   3.521 0.000453 ***\nfactor(county)16  0.66486    0.53487   1.243 0.214204    \nfactor(county)17  1.27480    0.38221   3.335 0.000890 ***\nfactor(county)18  1.12155    0.21913   5.118 3.83e-07 ***\nfactor(county)19  1.33831    0.09541  14.026  &lt; 2e-16 ***\nfactor(county)20  1.80032    0.43672   4.122 4.13e-05 ***\nfactor(county)21  1.73399    0.25227   6.873 1.23e-11 ***\nfactor(county)22  0.63679    0.30905   2.060 0.039663 *  \nfactor(county)23  1.39999    0.53613   2.611 0.009183 ** \nfactor(county)24  2.10162    0.25267   8.318 3.64e-16 ***\nfactor(county)25  1.95072    0.20243   9.636  &lt; 2e-16 ***\nfactor(county)26  1.36058    0.07422  18.332  &lt; 2e-16 ***\nfactor(county)27  1.77336    0.30978   5.725 1.45e-08 ***\nfactor(county)28  1.24159    0.34115   3.639 0.000290 ***\nfactor(county)29  1.05600    0.43672   2.418 0.015818 *  \nfactor(county)30  0.92576    0.22807   4.059 5.39e-05 ***\nfactor(county)31  2.02057    0.33828   5.973 3.45e-09 ***\nfactor(county)32  1.23629    0.37821   3.269 0.001124 ** \nfactor(county)33  2.06187    0.37821   5.452 6.58e-08 ***\nfactor(county)34  1.59044    0.43946   3.619 0.000314 ***\nfactor(county)35  0.81920    0.28897   2.835 0.004695 ** \nfactor(county)36  2.95897    0.53613   5.519 4.55e-08 ***\nfactor(county)37  0.40209    0.25227   1.594 0.111345    \nfactor(county)38  1.86772    0.37999   4.915 1.07e-06 ***\nfactor(county)39  1.74807    0.33860   5.163 3.05e-07 ***\nfactor(county)40  2.31580    0.37866   6.116 1.48e-09 ***\nfactor(county)41  1.96715    0.26759   7.351 4.69e-13 ***\nfactor(county)42  1.36098    0.75642   1.799 0.072343 .  \nfactor(county)43  1.60224    0.25543   6.273 5.69e-10 ***\nfactor(county)44  1.04099    0.28609   3.639 0.000291 ***\nfactor(county)45  1.29541    0.21101   6.139 1.28e-09 ***\nfactor(county)46  1.21461    0.33828   3.591 0.000349 ***\nfactor(county)47  0.88393    0.53613   1.649 0.099583 .  \nfactor(county)48  1.14812    0.25227   4.551 6.13e-06 ***\nfactor(county)49  1.70211    0.21010   8.102 1.93e-15 ***\nfactor(county)50  2.49321    0.75642   3.296 0.001022 ** \nfactor(county)51  2.16504    0.37821   5.724 1.45e-08 ***\nfactor(county)52  1.92769    0.43672   4.414 1.15e-05 ***\nfactor(county)53  1.25080    0.43741   2.860 0.004348 ** \nfactor(county)54  1.30676    0.15802   8.270 5.28e-16 ***\nfactor(county)55  1.61799    0.26885   6.018 2.64e-09 ***\nfactor(county)56  1.10110    0.43946   2.506 0.012415 *  \nfactor(county)57  0.76218    0.30905   2.466 0.013855 *  \nfactor(county)58  1.86092    0.37866   4.915 1.07e-06 ***\nfactor(county)59  1.72178    0.37999   4.531 6.73e-06 ***\nfactor(county)60  1.27939    0.53487   2.392 0.016979 *  \nfactor(county)61  1.15873    0.13389   8.654  &lt; 2e-16 ***\nfactor(county)62  1.98301    0.33860   5.856 6.80e-09 ***\nfactor(county)63  1.67070    0.43741   3.820 0.000144 ***\nfactor(county)64  1.84784    0.22817   8.099 1.97e-15 ***\nfactor(county)65  1.29912    0.53487   2.429 0.015357 *  \nfactor(county)66  1.66574    0.20648   8.067 2.50e-15 ***\nfactor(county)67  1.80312    0.21101   8.545  &lt; 2e-16 ***\nfactor(county)68  1.09002    0.26743   4.076 5.02e-05 ***\nfactor(county)69  1.24245    0.37821   3.285 0.001062 ** \nfactor(county)70  0.86763    0.07096  12.227  &lt; 2e-16 ***\nfactor(county)71  1.49184    0.15174   9.832  &lt; 2e-16 ***\nfactor(county)72  1.57990    0.23920   6.605 7.08e-11 ***\nfactor(county)73  1.79176    0.53487   3.350 0.000845 ***\nfactor(county)74  0.98704    0.37821   2.610 0.009223 ** \nfactor(county)75  1.72372    0.43741   3.941 8.80e-05 ***\nfactor(county)76  2.00844    0.37866   5.304 1.45e-07 ***\nfactor(county)77  1.82168    0.28609   6.367 3.17e-10 ***\nfactor(county)78  1.28569    0.33956   3.786 0.000164 ***\nfactor(county)79  0.61488    0.37866   1.624 0.104785    \nfactor(county)80  1.32952    0.11181  11.890  &lt; 2e-16 ***\nfactor(county)81  2.70953    0.43946   6.166 1.09e-09 ***\nfactor(county)82  2.23001    0.75642   2.948 0.003286 ** \nfactor(county)83  1.62292    0.21048   7.711 3.57e-14 ***\nfactor(county)84  1.64535    0.20987   7.840 1.38e-14 ***\nfactor(county)85  1.18652    0.53487   2.218 0.026801 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7564 on 833 degrees of freedom\nMultiple R-squared:  0.7671,    Adjusted R-squared:  0.7431 \nF-statistic: 31.91 on 86 and 833 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation:\n\nCounty-specific intercepts are provided for all 85 counties.\nThe R-squared appears inflated (\\(0.77\\)) compared to the no-pooling model (\\(0.29\\)), but this is due to how the lm() function calculates explained variance without a constant term.\nThe estimates for intercepts are consistent with the previous parameterization.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>No-Pooling Regression with No Constant Term</span>"
    ]
  },
  {
    "objectID": "cap16.html#setting-up-a-multilevel-regression-model-in-jags",
    "href": "cap16.html#setting-up-a-multilevel-regression-model-in-jags",
    "title": "No-Pooling Regression with No Constant Term",
    "section": "4.4 Setting up a Multilevel Regression Model in JAGS",
    "text": "4.4 Setting up a Multilevel Regression Model in JAGS\nWe can set up the multilevel model for the radon problem using JAGS (Just Another Gibbs Sampler) instead of Bugs. Below is the JAGS model code for a varying-intercept multilevel model.\n\n4.4.1 JAGS Model Code:\nSee radon_model.jags for the JAGS model code.\nExplanation:\n\nLikelihood: Each observation’s radon level, y[i], is modeled as normally distributed with mean y.hat[i], where y.hat[i] = a[county[i]] + b * x[i].\nPriors:\n\nThe slope b is given a noninformative prior (dnorm(0, 0.0001)).\nThe group-level intercepts a[j] are also normally distributed with mean mu.a and precision tau.a.\nsigma.y and sigma.a are assigned uniform priors.\n\n\n\n\n4.4.2 Running JAGS in R:\nOnce the model is set up, we can run it in R using the rjags package. Below is the R code to load the data, initialize the model, and run it:\n\n# Load the rjags package\nlibrary(rjags)\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.2\n\n\nLoaded modules: basemod,bugs\n\n# Prepare data list for JAGS\ndata_jags &lt;- list(y = y, x = x, county = county, n = n, J = J)\n\n# Initial values for JAGS\ninits &lt;- function() {\n  list(a = rnorm(J), b = rnorm(1), mu.a = rnorm(1), sigma.y = runif(1), sigma.a = runif(1))\n}\n\n# Parameters to monitor\nparams &lt;- c(\"a\", \"b\", \"mu.a\", \"sigma.y\", \"sigma.a\")\n\n# Run JAGS\njags_model &lt;- jags.model(\"codigoJAGS/radon_model.jags\", data = data_jags, inits = inits, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 919\n   Unobserved stochastic nodes: 89\n   Total graph size: 3002\n\nInitializing model\n\nupdate(jags_model, 1000)  # Burn-in\nsamples &lt;- coda.samples(jags_model, variable.names = params, n.iter = 5000)\nsummary(samples)\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean      SD  Naive SE Time-series SE\na[1]     1.1902 0.25472 0.0020798      0.0022215\na[2]     0.9291 0.10083 0.0008233      0.0008361\na[3]     1.4779 0.26628 0.0021741      0.0022989\na[4]     1.5040 0.21745 0.0017755      0.0018811\na[5]     1.4417 0.25021 0.0020430      0.0021261\na[6]     1.4753 0.26535 0.0021666      0.0022232\na[7]     1.8558 0.17646 0.0014408      0.0015735\na[8]     1.6801 0.25668 0.0020958      0.0025060\na[9]     1.1598 0.19610 0.0016012      0.0016943\na[10]    1.5108 0.22785 0.0018603      0.0020350\na[11]    1.4321 0.23965 0.0019567      0.0020334\na[12]    1.5790 0.25278 0.0020639      0.0020942\na[13]    1.2366 0.22850 0.0018657      0.0019578\na[14]    1.8342 0.17615 0.0014382      0.0016110\na[15]    1.3986 0.25350 0.0020699      0.0021287\na[16]    1.2385 0.28770 0.0023490      0.0026070\na[17]    1.3694 0.25055 0.0020457      0.0021395\na[18]    1.2192 0.18212 0.0014870      0.0015150\na[19]    1.3461 0.09248 0.0007551      0.0007736\na[20]    1.5849 0.26686 0.0021789      0.0022508\na[21]    1.6276 0.20239 0.0016525      0.0017372\na[22]    1.0171 0.23156 0.0018907      0.0022331\na[23]    1.4392 0.28458 0.0023236      0.0024335\na[24]    1.8562 0.20497 0.0016736      0.0019966\na[25]    1.8095 0.17553 0.0014332      0.0015704\na[26]    1.3624 0.07267 0.0005934      0.0006025\na[27]    1.6204 0.23127 0.0018883      0.0019514\na[28]    1.3438 0.23752 0.0019393      0.0020924\na[29]    1.3131 0.26548 0.0021676      0.0022049\na[30]    1.1020 0.19058 0.0015561      0.0017287\na[31]    1.7283 0.23986 0.0019585      0.0022789\na[32]    1.3642 0.25134 0.0020522      0.0020832\na[33]    1.7203 0.25707 0.0020989      0.0023097\na[34]    1.5033 0.26680 0.0021784      0.0022938\na[35]    1.0838 0.22364 0.0018260      0.0020182\na[36]    1.8703 0.29707 0.0024256      0.0032681\na[37]    0.7936 0.21445 0.0017510      0.0023798\na[38]    1.6329 0.25119 0.0020510      0.0021798\na[39]    1.5971 0.23590 0.0019261      0.0019550\na[40]    1.8253 0.25812 0.0021075      0.0025912\na[41]    1.7623 0.21274 0.0017370      0.0019427\na[42]    1.4419 0.30713 0.0025077      0.0026002\na[43]    1.5380 0.20427 0.0016678      0.0017839\na[44]    1.2196 0.21911 0.0017890      0.0018555\na[45]    1.3364 0.17767 0.0014506      0.0014270\na[46]    1.3385 0.23553 0.0019231      0.0019231\na[47]    1.2946 0.28614 0.0023364      0.0024909\na[48]    1.2608 0.20065 0.0016383      0.0016896\na[49]    1.6276 0.17944 0.0014651      0.0015340\na[50]    1.6267 0.30971 0.0025288      0.0027133\na[51]    1.7625 0.25521 0.0020837      0.0023811\na[52]    1.6295 0.26640 0.0021751      0.0023139\na[53]    1.3777 0.26518 0.0021652      0.0022612\na[54]    1.3338 0.14199 0.0011593      0.0012428\na[55]    1.5501 0.21048 0.0017185      0.0017661\na[56]    1.3250 0.26667 0.0021773      0.0022911\na[57]    1.0880 0.23122 0.0018879      0.0020944\na[58]    1.6279 0.25056 0.0020458      0.0021275\na[59]    1.5659 0.25447 0.0020777      0.0022161\na[60]    1.4085 0.28435 0.0023217      0.0023231\na[61]    1.2007 0.12456 0.0010170      0.0010335\na[62]    1.7110 0.24075 0.0019657      0.0022969\na[63]    1.5365 0.26591 0.0021712      0.0023192\na[64]    1.7182 0.19152 0.0015637      0.0016741\na[65]    1.4170 0.28323 0.0023125      0.0023126\na[66]    1.5952 0.17630 0.0014395      0.0016465\na[67]    1.6960 0.17954 0.0014660      0.0015156\na[68]    1.2403 0.20919 0.0017080      0.0017733\na[69]    1.3655 0.25088 0.0020484      0.0020736\na[70]    0.8914 0.06985 0.0005704      0.0005969\na[71]    1.4823 0.13819 0.0011283      0.0011263\na[72]    1.5365 0.19189 0.0015668      0.0015925\na[73]    1.5496 0.28372 0.0023166      0.0024045\na[74]    1.2546 0.25044 0.0020448      0.0021601\na[75]    1.5512 0.26872 0.0021941      0.0022134\na[76]    1.6948 0.25301 0.0020658      0.0022551\na[77]    1.6615 0.21668 0.0017692      0.0018926\na[78]    1.3706 0.23622 0.0019287      0.0019288\na[79]    1.0938 0.25453 0.0020782      0.0023901\na[80]    1.3384 0.10633 0.0008682      0.0008753\na[81]    1.9058 0.27815 0.0022711      0.0030650\na[82]    1.5851 0.30864 0.0025200      0.0028872\na[83]    1.5699 0.17585 0.0014358      0.0014512\na[84]    1.5885 0.17833 0.0014560      0.0014561\na[85]    1.3834 0.28339 0.0023138      0.0023463\nb       -0.6915 0.07081 0.0005782      0.0008687\nmu.a     1.4602 0.05286 0.0004316      0.0008124\nsigma.a  0.3320 0.04579 0.0003739      0.0010311\nsigma.y  0.7571 0.01862 0.0001520      0.0002177\n\n2. Quantiles for each variable:\n\n           2.5%     25%     50%     75%   97.5%\na[1]     0.6794  1.0235  1.1934  1.3614  1.6854\na[2]     0.7315  0.8611  0.9290  0.9962  1.1281\na[3]     0.9499  1.2999  1.4778  1.6562  1.9969\na[4]     1.0825  1.3584  1.4985  1.6525  1.9342\na[5]     0.9487  1.2743  1.4435  1.6085  1.9302\na[6]     0.9583  1.2960  1.4732  1.6549  2.0011\na[7]     1.5162  1.7359  1.8544  1.9731  2.2087\na[8]     1.1888  1.5048  1.6799  1.8496  2.1956\na[9]     0.7728  1.0304  1.1600  1.2918  1.5419\na[10]    1.0681  1.3577  1.5092  1.6641  1.9606\na[11]    0.9614  1.2736  1.4325  1.5911  1.9003\na[12]    1.0854  1.4083  1.5750  1.7494  2.0859\na[13]    0.7813  1.0842  1.2389  1.3920  1.6774\na[14]    1.5001  1.7153  1.8326  1.9523  2.1820\na[15]    0.8938  1.2289  1.3992  1.5681  1.8972\na[16]    0.6653  1.0495  1.2410  1.4347  1.7942\na[17]    0.8743  1.2012  1.3688  1.5336  1.8628\na[18]    0.8578  1.0977  1.2195  1.3421  1.5763\na[19]    1.1659  1.2840  1.3458  1.4075  1.5274\na[20]    1.0688  1.4044  1.5849  1.7626  2.1138\na[21]    1.2353  1.4919  1.6254  1.7634  2.0293\na[22]    0.5577  0.8622  1.0211  1.1744  1.4678\na[23]    0.8850  1.2510  1.4385  1.6305  1.9999\na[24]    1.4619  1.7179  1.8557  1.9920  2.2610\na[25]    1.4693  1.6907  1.8083  1.9288  2.1580\na[26]    1.2190  1.3126  1.3622  1.4113  1.5067\na[27]    1.1663  1.4673  1.6189  1.7754  2.0695\na[28]    0.8700  1.1859  1.3459  1.5049  1.8031\na[29]    0.7887  1.1354  1.3110  1.4942  1.8335\na[30]    0.7301  0.9727  1.1037  1.2328  1.4731\na[31]    1.2623  1.5682  1.7253  1.8859  2.2066\na[32]    0.8647  1.1961  1.3653  1.5330  1.8538\na[33]    1.2278  1.5451  1.7154  1.8891  2.2415\na[34]    0.9817  1.3236  1.5006  1.6832  2.0325\na[35]    0.6399  0.9347  1.0862  1.2367  1.5173\na[36]    1.3117  1.6684  1.8618  2.0657  2.4694\na[37]    0.3713  0.6494  0.7966  0.9381  1.2078\na[38]    1.1509  1.4616  1.6299  1.8016  2.1318\na[39]    1.1389  1.4383  1.5953  1.7565  2.0657\na[40]    1.3261  1.6504  1.8218  1.9941  2.3474\na[41]    1.3471  1.6189  1.7613  1.9039  2.1866\na[42]    0.8267  1.2393  1.4397  1.6441  2.0480\na[43]    1.1393  1.3997  1.5376  1.6748  1.9437\na[44]    0.7846  1.0757  1.2175  1.3688  1.6471\na[45]    0.9909  1.2158  1.3369  1.4582  1.6848\na[46]    0.8712  1.1821  1.3422  1.4948  1.7994\na[47]    0.7193  1.1089  1.2970  1.4847  1.8542\na[48]    0.8651  1.1270  1.2606  1.3954  1.6510\na[49]    1.2804  1.5083  1.6263  1.7491  1.9808\na[50]    1.0248  1.4215  1.6219  1.8299  2.2455\na[51]    1.2705  1.5900  1.7588  1.9335  2.2734\na[52]    1.1089  1.4509  1.6254  1.8074  2.1572\na[53]    0.8567  1.2015  1.3745  1.5546  1.9046\na[54]    1.0551  1.2390  1.3338  1.4289  1.6157\na[55]    1.1381  1.4099  1.5502  1.6900  1.9628\na[56]    0.8011  1.1451  1.3257  1.5039  1.8453\na[57]    0.6273  0.9345  1.0891  1.2468  1.5349\na[58]    1.1423  1.4608  1.6272  1.7936  2.1265\na[59]    1.0700  1.3951  1.5616  1.7354  2.0746\na[60]    0.8516  1.2177  1.4063  1.6001  1.9700\na[61]    0.9513  1.1178  1.2013  1.2847  1.4423\na[62]    1.2502  1.5467  1.7079  1.8727  2.1904\na[63]    1.0211  1.3583  1.5321  1.7113  2.0670\na[64]    1.3415  1.5880  1.7180  1.8463  2.0955\na[65]    0.8547  1.2286  1.4174  1.6052  1.9669\na[66]    1.2484  1.4766  1.5963  1.7129  1.9421\na[67]    1.3486  1.5741  1.6944  1.8169  2.0497\na[68]    0.8298  1.0993  1.2408  1.3799  1.6476\na[69]    0.8732  1.1958  1.3652  1.5347  1.8542\na[70]    0.7545  0.8439  0.8914  0.9391  1.0277\na[71]    1.2105  1.3891  1.4826  1.5756  1.7525\na[72]    1.1601  1.4072  1.5367  1.6662  1.9099\na[73]    0.9849  1.3608  1.5480  1.7387  2.1143\na[74]    0.7584  1.0856  1.2575  1.4244  1.7446\na[75]    1.0238  1.3699  1.5526  1.7308  2.0836\na[76]    1.2048  1.5252  1.6928  1.8602  2.2013\na[77]    1.2402  1.5171  1.6598  1.8066  2.0908\na[78]    0.9055  1.2132  1.3714  1.5297  1.8284\na[79]    0.5820  0.9274  1.0985  1.2637  1.5867\na[80]    1.1311  1.2667  1.3379  1.4097  1.5474\na[81]    1.3726  1.7159  1.8988  2.0865  2.4698\na[82]    0.9885  1.3783  1.5823  1.7863  2.2022\na[83]    1.2229  1.4521  1.5701  1.6883  1.9193\na[84]    1.2354  1.4709  1.5885  1.7085  1.9355\na[85]    0.8271  1.1961  1.3826  1.5721  1.9424\nb       -0.8293 -0.7397 -0.6919 -0.6433 -0.5532\nmu.a     1.3572  1.4244  1.4601  1.4961  1.5644\nsigma.a  0.2491  0.3004  0.3296  0.3616  0.4285\nsigma.y  0.7214  0.7443  0.7568  0.7694  0.7944\n\n\nDiagnósticos:\n\nprint(jags_model)\n\nJAGS model:\n\nmodel {\n  for (i in 1:n) {\n    y[i] ~ dnorm(y.hat[i], tau.y)\n    y.hat[i] &lt;- a[county[i]] + b * x[i]\n  }\n\n  # Priors\n  b ~ dnorm(0, 0.0001)\n  tau.y &lt;- pow(sigma.y, -2)\n  sigma.y ~ dunif(0, 100)\n\n  for (j in 1:J) {\n    a[j] ~ dnorm(mu.a, tau.a)\n  }\n\n  mu.a ~ dnorm(0, 0.0001)\n  tau.a &lt;- pow(sigma.a, -2)\n  sigma.a ~ dunif(0, 100)\n}\nFully observed variables:\n J county n x y \n\n\n\nlibrary(R2jags)\n\n\nAttaching package: 'R2jags'\n\n\nThe following object is masked from 'package:coda':\n\n    traceplot\n\nset.seed(1234)\njags_model2 &lt;-\n  jags(\n    model.file = \"codigoJAGS/radon_model.jags\", data = data_jags,parameters.to.save = c(\"a\", \"b\", \"mu.a\", \"sigma.y\", \"sigma.a\"),n.iter = 10000, n.burnin = 3000,n.thin = 2)\n\nmodule glm loaded\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 919\n   Unobserved stochastic nodes: 89\n   Total graph size: 3002\n\nInitializing model\n\n\n\njags_model2\n\nInference for Bugs model at \"codigoJAGS/radon_model.jags\", fit using jags,\n 3 chains, each with 10000 iterations (first 3000 discarded), n.thin = 2\n n.sims = 10500 iterations saved\n          mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat\na[1]        1.189   0.255    0.677    1.021    1.193    1.360    1.677 1.001\na[2]        0.927   0.101    0.727    0.859    0.928    0.995    1.125 1.001\na[3]        1.478   0.269    0.947    1.300    1.478    1.658    2.008 1.001\na[4]        1.502   0.218    1.066    1.355    1.501    1.647    1.931 1.001\na[5]        1.448   0.254    0.958    1.277    1.448    1.619    1.939 1.001\na[6]        1.486   0.267    0.971    1.308    1.483    1.665    2.014 1.001\na[7]        1.859   0.177    1.516    1.738    1.858    1.977    2.211 1.001\na[8]        1.685   0.257    1.189    1.516    1.680    1.856    2.199 1.001\na[9]        1.162   0.198    0.770    1.031    1.164    1.296    1.539 1.001\na[10]       1.511   0.226    1.064    1.358    1.511    1.664    1.963 1.001\na[11]       1.430   0.239    0.969    1.269    1.429    1.591    1.904 1.001\na[12]       1.577   0.252    1.086    1.407    1.572    1.746    2.080 1.001\na[13]       1.238   0.229    0.782    1.083    1.239    1.391    1.687 1.001\na[14]       1.836   0.177    1.491    1.716    1.833    1.953    2.191 1.001\na[15]       1.401   0.252    0.908    1.232    1.404    1.572    1.897 1.001\na[16]       1.236   0.287    0.657    1.048    1.239    1.426    1.793 1.001\na[17]       1.372   0.254    0.876    1.201    1.372    1.540    1.874 1.001\na[18]       1.220   0.184    0.867    1.096    1.218    1.342    1.584 1.002\na[19]       1.346   0.092    1.166    1.285    1.346    1.408    1.529 1.001\na[20]       1.584   0.268    1.055    1.404    1.583    1.761    2.109 1.001\na[21]       1.630   0.202    1.232    1.494    1.630    1.765    2.028 1.001\na[22]       1.020   0.236    0.550    0.865    1.024    1.181    1.464 1.001\na[23]       1.441   0.287    0.879    1.254    1.439    1.633    2.010 1.001\na[24]       1.864   0.207    1.463    1.722    1.864    2.002    2.272 1.001\na[25]       1.813   0.175    1.467    1.697    1.813    1.932    2.160 1.001\na[26]       1.364   0.073    1.223    1.314    1.363    1.414    1.510 1.001\na[27]       1.619   0.229    1.178    1.464    1.620    1.770    2.077 1.001\na[28]       1.351   0.239    0.879    1.193    1.352    1.510    1.824 1.001\na[29]       1.310   0.269    0.762    1.134    1.310    1.489    1.839 1.002\na[30]       1.100   0.191    0.728    0.970    1.098    1.230    1.471 1.001\na[31]       1.735   0.242    1.263    1.572    1.734    1.894    2.222 1.001\na[32]       1.363   0.250    0.867    1.196    1.365    1.534    1.849 1.001\na[33]       1.721   0.253    1.229    1.552    1.715    1.887    2.224 1.001\na[34]       1.499   0.266    0.975    1.320    1.498    1.677    2.020 1.001\na[35]       1.082   0.223    0.642    0.932    1.085    1.235    1.505 1.002\na[36]       1.883   0.304    1.300    1.682    1.875    2.078    2.511 1.001\na[37]       0.792   0.214    0.372    0.648    0.795    0.937    1.203 1.001\na[38]       1.630   0.258    1.120    1.458    1.629    1.800    2.149 1.001\na[39]       1.600   0.241    1.127    1.439    1.597    1.759    2.078 1.001\na[40]       1.832   0.258    1.341    1.658    1.828    1.999    2.357 1.001\na[41]       1.764   0.210    1.369    1.624    1.760    1.903    2.182 1.001\na[42]       1.441   0.313    0.823    1.235    1.440    1.647    2.059 1.001\na[43]       1.539   0.206    1.134    1.398    1.539    1.678    1.937 1.001\na[44]       1.219   0.220    0.779    1.074    1.219    1.368    1.649 1.001\na[45]       1.337   0.180    0.983    1.216    1.338    1.459    1.692 1.001\na[46]       1.343   0.239    0.876    1.184    1.345    1.501    1.808 1.001\na[47]       1.293   0.286    0.727    1.105    1.293    1.487    1.849 1.002\na[48]       1.261   0.202    0.867    1.127    1.263    1.399    1.657 1.001\na[49]       1.630   0.180    1.280    1.510    1.629    1.747    1.980 1.001\na[50]       1.627   0.312    1.027    1.411    1.624    1.831    2.248 1.001\na[51]       1.762   0.254    1.270    1.587    1.759    1.930    2.272 1.001\na[52]       1.634   0.265    1.130    1.452    1.629    1.812    2.170 1.001\na[53]       1.380   0.267    0.863    1.203    1.380    1.560    1.895 1.001\na[54]       1.331   0.142    1.054    1.235    1.329    1.428    1.609 1.001\na[55]       1.548   0.213    1.134    1.407    1.547    1.691    1.970 1.001\na[56]       1.320   0.268    0.783    1.142    1.320    1.500    1.844 1.001\na[57]       1.087   0.234    0.622    0.931    1.088    1.243    1.537 1.001\na[58]       1.627   0.251    1.138    1.460    1.627    1.795    2.124 1.001\na[59]       1.566   0.255    1.068    1.395    1.563    1.737    2.067 1.001\na[60]       1.409   0.286    0.839    1.219    1.413    1.597    1.968 1.001\na[61]       1.200   0.124    0.954    1.117    1.201    1.283    1.441 1.001\na[62]       1.715   0.242    1.241    1.550    1.714    1.874    2.200 1.001\na[63]       1.536   0.272    1.007    1.353    1.533    1.719    2.072 1.001\na[64]       1.721   0.190    1.350    1.593    1.722    1.846    2.094 1.001\na[65]       1.411   0.283    0.854    1.221    1.413    1.600    1.966 1.001\na[66]       1.595   0.176    1.247    1.478    1.596    1.714    1.943 1.001\na[67]       1.701   0.179    1.353    1.579    1.702    1.822    2.055 1.002\na[68]       1.240   0.209    0.826    1.100    1.241    1.379    1.654 1.001\na[69]       1.367   0.255    0.861    1.199    1.370    1.537    1.865 1.001\na[70]       0.891   0.070    0.752    0.844    0.891    0.939    1.028 1.001\na[71]       1.482   0.138    1.215    1.390    1.482    1.575    1.752 1.001\na[72]       1.536   0.194    1.155    1.405    1.537    1.667    1.914 1.001\na[73]       1.554   0.286    0.993    1.360    1.550    1.743    2.122 1.001\na[74]       1.256   0.251    0.756    1.091    1.256    1.423    1.750 1.001\na[75]       1.557   0.269    1.031    1.377    1.556    1.738    2.097 1.001\na[76]       1.698   0.255    1.205    1.527    1.694    1.866    2.207 1.001\na[77]       1.665   0.222    1.232    1.515    1.664    1.810    2.109 1.001\na[78]       1.369   0.238    0.901    1.210    1.369    1.528    1.841 1.001\na[79]       1.091   0.257    0.578    0.922    1.094    1.265    1.583 1.001\na[80]       1.339   0.107    1.128    1.269    1.340    1.411    1.546 1.001\na[81]       1.910   0.280    1.382    1.716    1.905    2.095    2.476 1.001\na[82]       1.585   0.311    0.972    1.376    1.585    1.788    2.195 1.001\na[83]       1.571   0.179    1.229    1.449    1.570    1.693    1.925 1.001\na[84]       1.591   0.177    1.243    1.471    1.593    1.707    1.943 1.001\na[85]       1.380   0.284    0.819    1.193    1.379    1.568    1.939 1.001\nb          -0.692   0.071   -0.831   -0.741   -0.693   -0.645   -0.551 1.001\nmu.a        1.461   0.052    1.361    1.426    1.460    1.496    1.565 1.001\nsigma.a     0.335   0.047    0.250    0.302    0.332    0.364    0.433 1.002\nsigma.y     0.756   0.019    0.721    0.744    0.756    0.769    0.794 1.001\ndeviance 2094.588  12.926 2070.831 2085.533 2094.154 2103.304 2120.918 1.001\n         n.eff\na[1]      4800\na[2]     10000\na[3]      7900\na[4]     10000\na[5]     10000\na[6]     10000\na[7]      5600\na[8]     10000\na[9]     10000\na[10]    10000\na[11]     4600\na[12]    10000\na[13]     7200\na[14]    10000\na[15]     4000\na[16]    10000\na[17]     5400\na[18]     2900\na[19]     5300\na[20]     9600\na[21]    10000\na[22]    10000\na[23]    10000\na[24]     9600\na[25]    10000\na[26]    10000\na[27]    10000\na[28]    10000\na[29]     2400\na[30]    10000\na[31]     6700\na[32]    10000\na[33]     5600\na[34]     6300\na[35]     8700\na[36]     9600\na[37]     8100\na[38]     4900\na[39]     7700\na[40]     8000\na[41]    10000\na[42]    10000\na[43]     5100\na[44]    10000\na[45]     6400\na[46]    10000\na[47]     2700\na[48]    10000\na[49]     6800\na[50]     5600\na[51]     4400\na[52]    10000\na[53]    10000\na[54]    10000\na[55]     8300\na[56]    10000\na[57]    10000\na[58]     6100\na[59]    10000\na[60]     3600\na[61]    10000\na[62]    10000\na[63]    10000\na[64]    10000\na[65]    10000\na[66]    10000\na[67]     3100\na[68]    10000\na[69]    10000\na[70]    10000\na[71]    10000\na[72]     8600\na[73]    10000\na[74]    10000\na[75]     4400\na[76]    10000\na[77]    10000\na[78]    10000\na[79]    10000\na[80]    10000\na[81]     4100\na[82]    10000\na[83]    10000\na[84]     3800\na[85]    10000\nb        10000\nmu.a     10000\nsigma.a   2800\nsigma.y   5500\ndeviance 10000\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 83.6 and DIC = 2178.1\nDIC is an estimate of expected predictive error (lower deviance is better).\n\n\n\nlibrary(CalvinBayes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: ggplot2\n\n\nLoading required package: ggformula\n\n\nLoading required package: scales\n\n\nLoading required package: ggridges\n\n\n\nNew to ggformula?  Try the tutorials: \n    learnr::run_tutorial(\"introduction\", package = \"ggformula\")\n    learnr::run_tutorial(\"refining\", package = \"ggformula\")\n\n\nLoading required package: bayesplot\n\n\nThis is bayesplot version 1.11.1\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\n\nAttaching package: 'CalvinBayes'\n\n\nThe following object is masked from 'package:bayesplot':\n\n    rhat\n\n\nThe following object is masked from 'package:datasets':\n\n    HairEyeColor\n\ndiagMCMC(samples , parName=\"a[1]\" )\n\n\n\n\n\n\n\ndiagMCMC(samples , parName=\"sigma.a\" )\n\n\n\n\n\n\n\n\nIn a Bayesian analysis run using Bugs or JAGS, the summary typically includes the means, standard deviations, and quantiles of all parameters, along with two key diagnostics for convergence:\n\nR-hat (Potential Scale Reduction Factor): This measures how much the parameter estimates would improve if the model were run indefinitely. R-hat ≤ 1.1 indicates good convergence.\nEffective Sample Size (n_eff): This metric reflects how many independent samples the Markov Chain Monte Carlo (MCMC) has effectively produced. A high n_eff value means the parameter estimates are reliable and not highly autocorrelated, leading to better precision. We usually like to have n_eff to be at least 100 for typical estimates and confidence intervals.\n\nIn JAGS, after running your model, the simulations (posterior samples) can be accessed in R. The saved MCMC object contains simulation draws for each parameter. For example, scalar parameters like b, mu.a, sigma.y, and sigma.a are vectors of length corresponding to the number of saved draws (e.g., 750). Parameters like a (which varies by county) will be represented as matrices.\nExample code for accessing and summarizing parameter estimates:\n\n#samples &lt;- coda.samples(jags_model2, variable.names = #params, n.iter = 5000)\n\n# Quantiles for b\nquantile(as.numeric(samples[[1]][,\"b\"]), c(0.05, 0.95))\n\n        5%        95% \n-0.8070339 -0.5766632 \n\n# Probability that radon levels in county 36 are higher than in county 26\nmean(samples[[1]][,'a[36]'] &gt; samples[[1]][,'a[26]'])\n\n[1] 0.9554\n\n\nFitted Values, Residuals, and Other Calculations in JAGS\nIn JAGS, you can calculate fitted values and residuals after running the model by combining parameter samples with the observed data. For instance:\n\n# Calculate fitted values and residuals\n## y.hat &lt;- a.multilevel[county] + b.multilevel * x\n## y.resid &lt;- y - y.hat\n\n# Plot residuals\n## plot(y.hat, y.resid)\n\nYou can also add y.hat as a monitored parameter during the JAGS run. For predictive checks, such as comparing radon levels between two counties:\n\nb_samples &lt;- as.numeric(samples[[1]][,\"b\"])\na36_samples &lt;- as.numeric(samples[[1]][,\"a[36]\"])\na26_samples &lt;- as.numeric(samples[[1]][,\"a[26]\"])\nsigma.y &lt;- as.numeric(samples[[1]][,\"sigma.y\"])\n# Predictive distribution for radon levels\nlqp.radon &lt;- exp(rnorm(n = 5000, a36_samples + b_samples, sigma.y))\nhennepin.radon &lt;- exp(rnorm(n = 5000, rnorm(1, a26_samples + b_samples, sigma.y)))\nradon.diff &lt;- lqp.radon - hennepin.radon\nhist(radon.diff)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>No-Pooling Regression with No Constant Term</span>"
    ]
  },
  {
    "objectID": "cap16.html#adding-individual--and-group-level-predictors-in-jags",
    "href": "cap16.html#adding-individual--and-group-level-predictors-in-jags",
    "title": "No-Pooling Regression with No Constant Term",
    "section": "4.5 Adding Individual- and Group-Level Predictors in JAGS",
    "text": "4.5 Adding Individual- and Group-Level Predictors in JAGS\nIn JAGS, we can add both individual- and group-level predictors to the model. For complete-pooling regression, where no group-level variation is considered, a simple linear regression can be fit using the following JAGS model:\n\n## JAGS model for complete pooling\n# model {\n#   for (i in 1:n) {\n#     y[i] ~ dnorm(y.hat[i], tau.y)\n#     y.hat[i] &lt;- a + b * x[i]  # Complete-pooling regression\n#   }\n#   \n#   # Priors\n#   a ~ dnorm(0, 0.0001)\n#   b ~ dnorm(0, 0.0001)\n#   tau.y &lt;- pow(sigma.y, -2)\n#   sigma.y ~ dunif(0, 100)\n# }\n\nExplanation:\n\ny[i] is the radon level for house i.\nx[i] is the basement status (individual-level predictor).\na is the intercept, and b is the slope.\n\nNo-Pooling Model in JAGS\nIn a no-pooling model, the intercepts for each county are allowed to vary, but no information is shared between them (i.e., no hierarchical structure). The JAGS model allows for county-specific intercepts (a[county[i]]) while maintaining a common slope (b).\n\n# # JAGS model for no-pooling\n# model {\n#   for (i in 1:n) {\n#     y[i] ~ dnorm(y.hat[i], tau.y)\n#     y.hat[i] &lt;- a[county[i]] + b * x[i]  # No-pooling model\n#   }\n#   \n#   # Priors\n#   b ~ dnorm(0, 0.0001)\n#   tau.y &lt;- pow(sigma.y, -2)\n#   sigma.y ~ dunif(0, 100)\n#   \n#   for (j in 1:J) {\n#     a[j] ~ dnorm(0, 0.0001)  # County-specific intercepts\n#   }\n# }\n\nExplanation:\n\na[j]: intercept for county j.\nb: common slope for the basement status.\n\nClassical Regression with Multiple Predictors in JAGS\nIn a classical regression with multiple predictors, you can extend the model to include additional covariates like whether the radon measurement was taken in winter. The model can also include interaction terms.\n\n# # JAGS model for classical regression with multiple predictors\n# model {\n#   for (i in 1:n) {\n#     y[i] ~ dnorm(y.hat[i], tau.y)\n#     y.hat[i] &lt;- a + b[1]*x[i] + b[2]*winter[i] + b[3]*x[i]*winter[i]  # Multiple predictors and interaction\n#   }\n#   \n#   # Priors for regression coefficients\n#   for (k in 1:K) {\n#     b[k] ~ dnorm(0, 0.0001)\n#   }\n#   \n#   # Other priors\n#   a ~ dnorm(0, 0.0001)\n#   tau.y &lt;- pow(sigma.y, -2)\n#   sigma.y ~ dunif(0, 100)\n# }\n\nExplanation:\n\nb[1], b[2], b[3]: coefficients for x, winter, and their interaction.\na: intercept.\nwinter[i]: an indicator for whether the measurement was taken in winter.\n\nVector-Matrix Notation in JAGS\nTo efficiently handle multiple predictors in JAGS, you can use vector-matrix notation. First, in R, create a matrix of predictors:\n\n# # Create a matrix of predictors in R\n# X &lt;- cbind(x, winter, x * winter)\n# K &lt;- ncol(X)\n\nThen, in the JAGS model, use the inner-product function for the linear predictor:\n\n# # JAGS model with vector-matrix notation\n# model {\n#   for (i in 1:n) {\n#     y[i] ~ dnorm(y.hat[i], tau.y)\n#     y.hat[i] &lt;- a + inprod(b[], X[i,])\n#   }\n# \n#   # Priors for regression coefficients\n#   for (k in 1:K) {\n#     b[k] ~ dnorm(0, 0.0001)\n#   }\n# \n#   # Other priors\n#   a ~ dnorm(0, 0.0001)\n#   tau.y &lt;- pow(sigma.y, -2)\n#   sigma.y ~ dunif(0, 100)\n# }\n\nIf you want to include the intercept in the matrix, prepend a vector of ones to the predictor matrix:\n\n# # Include intercept in the predictor matrix\n# ones &lt;- rep(1, n)\n# X &lt;- cbind(ones, x, winter, x * winter)\n# K &lt;- ncol(X)\n\nIn the JAGS model, simplify y.hat by using inprod(b[], X[i,]), where the coefficients b[1], …, b[4] correspond to the intercept and other predictors. This approach makes handling multiple predictors more efficient.\nMultilevel Model with a Group-Level Predictor in JAGS\nIn this multilevel model, a group-level predictor (such as uranium levels) influences the county-specific intercepts. Here’s the equivalent JAGS model:\nPreparación de datos:\n\nsrrs2.fips &lt;- srrs2$stfips*1000 + srrs2$cntyfips\ncty &lt;- read.table (\"data/ARM_Data/radon/cty.dat\", header=T, sep=\",\")\nusa.fips &lt;- 1000*cty[,\"stfips\"] + cty[,\"ctfips\"]\nusa.rows &lt;- match (unique(srrs2.fips[mn]), usa.fips)\nuranium &lt;- cty[usa.rows,\"Uppm\"]\nu &lt;- log (uranium)\n\nu.full &lt;- u[county]\n\n\ndata_jags$u &lt;- u.full\n\njags_model2_u &lt;-\n  jags(\n    model.file = \"codigoJAGS/radon_model_u.jags\", data = data_jags,parameters.to.save = c(\"a\", \"b\", \"sigma.y\", \"sigma.a\",\"g.0\",\"g.1\"),n.iter = 3000, n.burnin = 1000,n.thin = 2)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 919\n   Unobserved stochastic nodes: 90\n   Total graph size: 3936\n\nInitializing model\n\n\n\njags_model2_u\n\nInference for Bugs model at \"codigoJAGS/radon_model_u.jags\", fit using jags,\n 3 chains, each with 3000 iterations (first 1000 discarded), n.thin = 2\n n.sims = 3000 iterations saved\n          mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat\na[1]        1.175   0.256    0.670    1.005    1.178    1.349    1.676 1.002\na[2]        0.929   0.102    0.730    0.858    0.927    0.999    1.129 1.001\na[3]        1.479   0.271    0.947    1.303    1.477    1.656    2.013 1.001\na[4]        1.506   0.224    1.054    1.360    1.507    1.654    1.938 1.001\na[5]        1.432   0.256    0.932    1.257    1.432    1.603    1.939 1.001\na[6]        1.475   0.268    0.943    1.295    1.474    1.650    2.009 1.001\na[7]        1.861   0.177    1.524    1.739    1.861    1.978    2.207 1.001\na[8]        1.683   0.256    1.193    1.504    1.680    1.853    2.172 1.001\na[9]        1.148   0.201    0.742    1.013    1.151    1.281    1.546 1.001\na[10]       1.499   0.233    1.046    1.345    1.495    1.661    1.952 1.001\na[11]       1.427   0.241    0.947    1.266    1.431    1.587    1.892 1.002\na[12]       1.580   0.258    1.088    1.408    1.575    1.754    2.091 1.001\na[13]       1.226   0.234    0.757    1.070    1.228    1.385    1.671 1.001\na[14]       1.835   0.178    1.480    1.713    1.837    1.953    2.182 1.001\na[15]       1.391   0.250    0.895    1.222    1.396    1.560    1.867 1.001\na[16]       1.219   0.300    0.586    1.030    1.225    1.419    1.785 1.002\na[17]       1.363   0.255    0.867    1.190    1.361    1.538    1.867 1.001\na[18]       1.214   0.190    0.828    1.092    1.215    1.339    1.581 1.001\na[19]       1.347   0.093    1.165    1.284    1.346    1.408    1.532 1.002\na[20]       1.585   0.274    1.034    1.402    1.583    1.767    2.131 1.001\na[21]       1.627   0.204    1.228    1.489    1.628    1.765    2.034 1.001\na[22]       1.012   0.235    0.546    0.857    1.010    1.165    1.475 1.001\na[23]       1.433   0.293    0.839    1.244    1.430    1.628    1.990 1.001\na[24]       1.859   0.212    1.454    1.716    1.855    2.004    2.266 1.002\na[25]       1.811   0.178    1.460    1.688    1.810    1.930    2.168 1.002\na[26]       1.364   0.071    1.223    1.317    1.362    1.413    1.498 1.003\na[27]       1.621   0.234    1.156    1.469    1.620    1.778    2.081 1.001\na[28]       1.338   0.238    0.883    1.179    1.341    1.492    1.806 1.002\na[29]       1.306   0.267    0.767    1.128    1.308    1.491    1.824 1.001\na[30]       1.094   0.189    0.725    0.968    1.093    1.217    1.475 1.002\na[31]       1.734   0.244    1.275    1.572    1.723    1.893    2.217 1.003\na[32]       1.357   0.251    0.852    1.190    1.360    1.525    1.852 1.001\na[33]       1.723   0.260    1.236    1.543    1.719    1.893    2.242 1.001\na[34]       1.497   0.271    0.964    1.312    1.499    1.669    2.026 1.002\na[35]       1.079   0.230    0.634    0.933    1.079    1.235    1.534 1.001\na[36]       1.883   0.295    1.329    1.672    1.881    2.095    2.474 1.001\na[37]       0.787   0.218    0.343    0.648    0.791    0.934    1.208 1.002\na[38]       1.636   0.260    1.127    1.463    1.634    1.812    2.150 1.001\na[39]       1.589   0.243    1.111    1.429    1.590    1.753    2.066 1.001\na[40]       1.826   0.260    1.322    1.641    1.829    1.999    2.351 1.001\na[41]       1.763   0.215    1.356    1.615    1.760    1.908    2.191 1.001\na[42]       1.432   0.313    0.815    1.220    1.434    1.632    2.055 1.001\na[43]       1.539   0.202    1.134    1.405    1.539    1.674    1.945 1.001\na[44]       1.216   0.217    0.802    1.066    1.216    1.364    1.634 1.001\na[45]       1.333   0.181    0.979    1.212    1.332    1.457    1.685 1.001\na[46]       1.329   0.239    0.858    1.171    1.330    1.488    1.798 1.002\na[47]       1.284   0.296    0.685    1.097    1.280    1.476    1.878 1.002\na[48]       1.257   0.206    0.859    1.121    1.259    1.391    1.660 1.001\na[49]       1.625   0.176    1.281    1.504    1.625    1.742    1.968 1.001\na[50]       1.630   0.318    1.025    1.418    1.630    1.832    2.284 1.002\na[51]       1.772   0.258    1.274    1.595    1.768    1.942    2.308 1.001\na[52]       1.633   0.275    1.101    1.449    1.629    1.812    2.185 1.001\na[53]       1.365   0.273    0.831    1.182    1.366    1.544    1.909 1.002\na[54]       1.331   0.139    1.062    1.237    1.329    1.429    1.601 1.003\na[55]       1.548   0.210    1.152    1.403    1.543    1.687    1.975 1.000\na[56]       1.309   0.276    0.756    1.135    1.315    1.493    1.839 1.003\na[57]       1.085   0.234    0.603    0.936    1.088    1.243    1.529 1.001\na[58]       1.643   0.255    1.164    1.466    1.637    1.812    2.165 1.003\na[59]       1.577   0.256    1.083    1.405    1.580    1.747    2.086 1.003\na[60]       1.409   0.287    0.841    1.216    1.402    1.606    1.971 1.001\na[61]       1.201   0.123    0.962    1.115    1.201    1.286    1.435 1.001\na[62]       1.718   0.239    1.269    1.555    1.705    1.874    2.212 1.001\na[63]       1.533   0.265    0.998    1.349    1.533    1.707    2.062 1.001\na[64]       1.721   0.186    1.359    1.597    1.723    1.842    2.094 1.001\na[65]       1.416   0.286    0.871    1.218    1.414    1.607    1.988 1.001\na[66]       1.602   0.173    1.269    1.483    1.603    1.719    1.941 1.001\na[67]       1.702   0.183    1.353    1.575    1.697    1.826    2.072 1.001\na[68]       1.242   0.210    0.823    1.105    1.241    1.382    1.658 1.002\na[69]       1.373   0.255    0.882    1.202    1.379    1.540    1.865 1.001\na[70]       0.890   0.070    0.756    0.841    0.889    0.938    1.026 1.001\na[71]       1.484   0.137    1.209    1.394    1.488    1.576    1.743 1.001\na[72]       1.550   0.194    1.182    1.423    1.551    1.678    1.926 1.002\na[73]       1.567   0.302    0.980    1.370    1.564    1.768    2.141 1.001\na[74]       1.268   0.263    0.742    1.090    1.267    1.449    1.766 1.001\na[75]       1.572   0.275    1.034    1.392    1.564    1.754    2.120 1.001\na[76]       1.713   0.273    1.181    1.528    1.714    1.898    2.246 1.001\na[77]       1.680   0.220    1.241    1.536    1.683    1.830    2.105 1.001\na[78]       1.383   0.241    0.922    1.218    1.390    1.543    1.856 1.001\na[79]       1.093   0.268    0.571    0.914    1.099    1.277    1.603 1.002\na[80]       1.345   0.106    1.129    1.272    1.348    1.415    1.551 1.001\na[81]       1.943   0.292    1.381    1.747    1.937    2.134    2.538 1.001\na[82]       1.622   0.329    0.966    1.400    1.624    1.836    2.260 1.001\na[83]       1.584   0.184    1.232    1.458    1.584    1.706    1.948 1.001\na[84]       1.597   0.181    1.239    1.478    1.596    1.717    1.957 1.001\na[85]       1.397   0.288    0.846    1.197    1.404    1.590    1.958 1.001\nb          -0.691   0.070   -0.830   -0.739   -0.691   -0.644   -0.554 1.003\ng.0         1.477   0.081    1.320    1.424    1.477    1.530    1.637 1.001\ng.1         0.029   0.111   -0.188   -0.047    0.028    0.104    0.250 1.002\nsigma.a     0.340   0.048    0.253    0.305    0.338    0.371    0.439 1.001\nsigma.y     0.756   0.019    0.721    0.743    0.756    0.769    0.794 1.004\ndeviance 2094.198  12.884 2070.618 2085.284 2093.794 2102.373 2120.783 1.001\n         n.eff\na[1]      1200\na[2]      3000\na[3]      2900\na[4]      2900\na[5]      3000\na[6]      2800\na[7]      3000\na[8]      3000\na[9]      3000\na[10]     2200\na[11]     1900\na[12]     3000\na[13]     3000\na[14]     3000\na[15]     3000\na[16]     1600\na[17]     2200\na[18]     3000\na[19]     1700\na[20]     3000\na[21]     3000\na[22]     2300\na[23]     2900\na[24]     1400\na[25]     1900\na[26]      970\na[27]     2400\na[28]     1900\na[29]     3000\na[30]     1300\na[31]      790\na[32]     3000\na[33]     3000\na[34]     1400\na[35]     3000\na[36]     3000\na[37]     2500\na[38]     3000\na[39]     2700\na[40]     3000\na[41]     2900\na[42]     3000\na[43]     3000\na[44]     3000\na[45]     2900\na[46]     1700\na[47]     3000\na[48]     2300\na[49]     3000\na[50]     1300\na[51]     3000\na[52]     3000\na[53]     1200\na[54]      910\na[55]     3000\na[56]      850\na[57]     3000\na[58]      950\na[59]      850\na[60]     2200\na[61]     3000\na[62]     3000\na[63]     2700\na[64]     3000\na[65]     3000\na[66]     3000\na[67]     3000\na[68]     1700\na[69]     2300\na[70]     3000\na[71]     3000\na[72]     1800\na[73]     3000\na[74]     3000\na[75]     2900\na[76]     3000\na[77]     3000\na[78]     3000\na[79]     1900\na[80]     3000\na[81]     3000\na[82]     2600\na[83]     3000\na[84]     3000\na[85]     3000\nb          850\ng.0       2600\ng.1       1400\nsigma.a   3000\nsigma.y    570\ndeviance  3000\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 83.0 and DIC = 2177.2\nDIC is an estimate of expected predictive error (lower deviance is better).\n\n\nExplanation:\n\nu[j]: group-level predictor (e.g., county-level uranium).\na[j]: county-specific intercept influenced by the group-level predictor.\ng.0, g.1: regression coefficients for the group-level predictor.\n\nThis model accounts for both individual- and group-level variations.\nPredictions for New Observations and New Groups in JAGS\nPredicting a New Unit in an Existing Group:\nTo predict for a new house in an existing group, extend the dataset in R by adding an NA for the new observation. For example, to predict radon levels in a new house in county 26 without a basement:\n\n# Extend dataset\nn &lt;- n + 1\ny &lt;- c(y, NA)\ncounty &lt;- c(county, 26)\nx &lt;- c(x, 1)\n\nAdd the following line in the JAGS model to flag the predicted value:\n\n# y.tilde &lt;- y[n]\n\nThen run the model, saving the predicted value:\n\ndata_jags_pred &lt;- list(y = y, x = x, county = county, n = n, J = J)\n\njags_model3 &lt;-\n  jags(\n    model.file = \"codigoJAGS/radon_model_pred1.jags\", data = data_jags_pred,parameters.to.save = c(\"a\", \"b\", \"mu.a\", \"sigma.y\", \"sigma.a\",\"y.tilde\"),n.iter = 3000, n.burnin = 1000,n.thin = 2)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 919\n   Unobserved stochastic nodes: 90\n   Total graph size: 3005\n\nInitializing model\n\n# radon.parameters &lt;- c(radon.parameters, \"y.tilde\")\n\nTo summarize predictions:\n\njags_model3_mcmc &lt;- as.mcmc(jags_model3)\n\ndiag_mcmc(jags_model3_mcmc, parName = \"y.tilde\")\n\n\n\n\n\n\n\nquantile(exp(jags_model3_mcmc[,\"y.tilde\"][[1]]), c(0.25, 0.75))  # 50% confidence interval\n\n     25%      75% \n1.158621 3.196751 \n\n\nPredicting a New Unit in a New Group:\nFor predictions in a new group (e.g., a new county with no previous radon data), you can add a new county to the dataset. First, define the group-level predictor (e.g., average uranium level):\n\nu.tilde &lt;- mean(u)  # Group-level predictor for new county\n\nThen extend the dataset for the new group:\n\nn &lt;- n + 1\ny &lt;- c(y, NA)\ncounty &lt;- c(county, J + 1)\nx &lt;- c(x, 1)\nJ &lt;- J + 1\nu &lt;- c(u, u.tilde)\n\ndata_jags_pred2 &lt;- list(y = y, x = x, county = county, n = n, J = J, u = u)\n\njags_model4 &lt;-\n  jags(\n    model.file = \"codigoJAGS/radon_model_pred2.jags\", data = data_jags_pred2,parameters.to.save = c(\"a\", \"b\", \"mu.a\", \"sigma.y\", \"sigma.a\",\"y.tilde\",\"y.tilde2\"),n.iter = 3000, n.burnin = 1000,n.thin = 2)\n\nWarning in jags.model(model.file, data = data, inits = init.values, n.chains =\nn.chains, : Unused variable \"u\" in data\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 919\n   Unobserved stochastic nodes: 92\n   Total graph size: 3010\n\nInitializing model\n\n\n\njags_model4\n\nInference for Bugs model at \"codigoJAGS/radon_model_pred2.jags\", fit using jags,\n 3 chains, each with 3000 iterations (first 1000 discarded), n.thin = 2\n n.sims = 3000 iterations saved\n          mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat\na[1]        1.192   0.257    0.686    1.020    1.194    1.363    1.683 1.001\na[2]        0.926   0.099    0.733    0.860    0.927    0.992    1.121 1.001\na[3]        1.477   0.280    0.939    1.292    1.475    1.666    2.028 1.001\na[4]        1.506   0.217    1.082    1.361    1.506    1.645    1.943 1.001\na[5]        1.454   0.250    0.963    1.289    1.454    1.618    1.952 1.001\na[6]        1.486   0.268    0.948    1.313    1.484    1.665    2.001 1.002\na[7]        1.859   0.176    1.517    1.737    1.859    1.980    2.206 1.001\na[8]        1.686   0.257    1.193    1.511    1.687    1.853    2.195 1.001\na[9]        1.167   0.195    0.777    1.033    1.169    1.298    1.534 1.001\na[10]       1.515   0.233    1.065    1.358    1.516    1.674    1.977 1.001\na[11]       1.430   0.240    0.952    1.272    1.429    1.595    1.891 1.001\na[12]       1.581   0.253    1.089    1.414    1.580    1.745    2.096 1.001\na[13]       1.239   0.229    0.776    1.087    1.241    1.395    1.682 1.001\na[14]       1.837   0.177    1.480    1.720    1.838    1.953    2.180 1.001\na[15]       1.395   0.255    0.903    1.224    1.399    1.560    1.898 1.001\na[16]       1.230   0.289    0.650    1.045    1.236    1.422    1.772 1.003\na[17]       1.371   0.253    0.865    1.207    1.369    1.538    1.856 1.003\na[18]       1.222   0.185    0.863    1.098    1.221    1.346    1.576 1.001\na[19]       1.345   0.092    1.165    1.283    1.345    1.408    1.522 1.002\na[20]       1.592   0.277    1.055    1.403    1.584    1.778    2.158 1.001\na[21]       1.629   0.202    1.223    1.492    1.628    1.767    2.035 1.001\na[22]       1.012   0.234    0.536    0.854    1.012    1.172    1.469 1.001\na[23]       1.450   0.288    0.881    1.262    1.452    1.633    2.024 1.001\na[24]       1.864   0.202    1.472    1.728    1.859    1.999    2.269 1.002\na[25]       1.817   0.174    1.471    1.702    1.816    1.933    2.156 1.001\na[26]       1.362   0.074    1.215    1.311    1.359    1.410    1.514 1.001\na[27]       1.632   0.235    1.159    1.476    1.638    1.788    2.084 1.003\na[28]       1.346   0.242    0.886    1.186    1.344    1.510    1.831 1.002\na[29]       1.305   0.269    0.779    1.128    1.315    1.487    1.833 1.001\na[30]       1.098   0.191    0.729    0.968    1.098    1.228    1.476 1.001\na[31]       1.738   0.243    1.268    1.577    1.731    1.904    2.207 1.001\na[32]       1.358   0.250    0.861    1.190    1.361    1.530    1.845 1.002\na[33]       1.721   0.251    1.232    1.547    1.721    1.888    2.233 1.001\na[34]       1.503   0.265    0.969    1.329    1.504    1.681    2.007 1.001\na[35]       1.084   0.223    0.638    0.941    1.088    1.237    1.515 1.001\na[36]       1.879   0.308    1.295    1.670    1.866    2.083    2.511 1.001\na[37]       0.783   0.211    0.367    0.647    0.783    0.925    1.189 1.002\na[38]       1.633   0.249    1.146    1.470    1.624    1.798    2.146 1.002\na[39]       1.601   0.240    1.123    1.438    1.597    1.758    2.078 1.001\na[40]       1.838   0.262    1.350    1.658    1.838    2.015    2.372 1.001\na[41]       1.771   0.214    1.358    1.626    1.769    1.917    2.186 1.002\na[42]       1.444   0.308    0.844    1.233    1.452    1.653    2.050 1.001\na[43]       1.544   0.204    1.144    1.405    1.544    1.684    1.942 1.001\na[44]       1.215   0.217    0.780    1.071    1.217    1.362    1.624 1.001\na[45]       1.344   0.176    1.005    1.227    1.347    1.461    1.689 1.001\na[46]       1.338   0.238    0.866    1.178    1.341    1.499    1.805 1.001\na[47]       1.289   0.289    0.722    1.097    1.286    1.485    1.856 1.002\na[48]       1.264   0.200    0.881    1.129    1.262    1.405    1.646 1.001\na[49]       1.632   0.177    1.285    1.513    1.627    1.750    1.988 1.001\na[50]       1.626   0.317    1.017    1.411    1.626    1.821    2.285 1.001\na[51]       1.773   0.258    1.274    1.597    1.776    1.946    2.294 1.001\na[52]       1.639   0.267    1.113    1.457    1.634    1.816    2.159 1.001\na[53]       1.378   0.266    0.844    1.202    1.382    1.558    1.878 1.001\na[54]       1.332   0.143    1.045    1.238    1.335    1.427    1.613 1.002\na[55]       1.547   0.212    1.129    1.403    1.546    1.692    1.957 1.001\na[56]       1.321   0.269    0.780    1.138    1.322    1.501    1.843 1.001\na[57]       1.082   0.235    0.625    0.919    1.081    1.246    1.529 1.002\na[58]       1.632   0.251    1.149    1.461    1.627    1.798    2.130 1.002\na[59]       1.564   0.255    1.070    1.393    1.568    1.736    2.056 1.001\na[60]       1.412   0.290    0.830    1.218    1.415    1.615    1.968 1.001\na[61]       1.195   0.125    0.945    1.111    1.197    1.280    1.443 1.002\na[62]       1.716   0.241    1.244    1.554    1.716    1.879    2.192 1.001\na[63]       1.541   0.274    1.005    1.358    1.540    1.725    2.076 1.001\na[64]       1.721   0.188    1.355    1.596    1.719    1.841    2.100 1.002\na[65]       1.428   0.288    0.858    1.244    1.432    1.618    1.997 1.001\na[66]       1.595   0.178    1.251    1.478    1.593    1.711    1.942 1.001\na[67]       1.694   0.183    1.347    1.571    1.692    1.817    2.044 1.002\na[68]       1.236   0.209    0.827    1.095    1.237    1.374    1.645 1.001\na[69]       1.372   0.252    0.878    1.203    1.367    1.539    1.879 1.001\na[70]       0.890   0.070    0.751    0.841    0.889    0.935    1.028 1.001\na[71]       1.478   0.136    1.208    1.387    1.476    1.570    1.751 1.001\na[72]       1.542   0.197    1.154    1.411    1.542    1.671    1.927 1.001\na[73]       1.549   0.287    0.988    1.362    1.545    1.738    2.122 1.001\na[74]       1.253   0.249    0.745    1.093    1.256    1.422    1.726 1.001\na[75]       1.560   0.271    1.037    1.382    1.564    1.738    2.096 1.001\na[76]       1.701   0.257    1.213    1.521    1.698    1.874    2.214 1.001\na[77]       1.670   0.221    1.238    1.514    1.672    1.820    2.095 1.001\na[78]       1.377   0.241    0.910    1.219    1.375    1.535    1.854 1.001\na[79]       1.089   0.259    0.569    0.912    1.091    1.268    1.581 1.001\na[80]       1.334   0.105    1.135    1.264    1.333    1.404    1.540 1.001\na[81]       1.930   0.289    1.382    1.729    1.926    2.121    2.522 1.002\na[82]       1.595   0.316    0.987    1.381    1.592    1.801    2.227 1.001\na[83]       1.576   0.182    1.204    1.460    1.579    1.698    1.928 1.001\na[84]       1.593   0.179    1.237    1.474    1.590    1.712    1.948 1.001\na[85]       1.376   0.287    0.799    1.183    1.379    1.571    1.929 1.002\na[86]       1.466   0.344    0.795    1.235    1.467    1.695    2.157 1.001\nb          -0.693   0.070   -0.829   -0.738   -0.692   -0.646   -0.557 1.001\nmu.a        1.463   0.054    1.356    1.427    1.463    1.499    1.570 1.002\nsigma.a     0.338   0.047    0.252    0.305    0.337    0.368    0.438 1.001\nsigma.y     0.756   0.018    0.721    0.744    0.756    0.769    0.794 1.001\ny.tilde     0.688   0.762   -0.842    0.189    0.687    1.191    2.177 1.001\ny.tilde2    0.751   0.843   -0.860    0.180    0.741    1.321    2.366 1.001\ndeviance 2093.806  12.916 2070.191 2084.888 2093.284 2102.345 2120.004 1.001\n         n.eff\na[1]      3000\na[2]      3000\na[3]      3000\na[4]      3000\na[5]      3000\na[6]      3000\na[7]      3000\na[8]      3000\na[9]      3000\na[10]     3000\na[11]     3000\na[12]     3000\na[13]     3000\na[14]     2800\na[15]     3000\na[16]     3000\na[17]      820\na[18]     3000\na[19]     1600\na[20]     3000\na[21]     3000\na[22]     3000\na[23]     2500\na[24]     1900\na[25]     3000\na[26]     2700\na[27]      780\na[28]     1800\na[29]     3000\na[30]     3000\na[31]     3000\na[32]     2900\na[33]     2900\na[34]     3000\na[35]     3000\na[36]     3000\na[37]     1300\na[38]     1500\na[39]     3000\na[40]     3000\na[41]     2000\na[42]     3000\na[43]     2300\na[44]     3000\na[45]     3000\na[46]     3000\na[47]     1100\na[48]     2600\na[49]     3000\na[50]     3000\na[51]     2600\na[52]     3000\na[53]     3000\na[54]     3000\na[55]     3000\na[56]     3000\na[57]     2000\na[58]     1800\na[59]     3000\na[60]     3000\na[61]     1100\na[62]     3000\na[63]     3000\na[64]     1400\na[65]     3000\na[66]     2000\na[67]     1900\na[68]     2300\na[69]     3000\na[70]     3000\na[71]     3000\na[72]     3000\na[73]     3000\na[74]     3000\na[75]     3000\na[76]     2300\na[77]     3000\na[78]     2600\na[79]     2600\na[80]     3000\na[81]     1900\na[82]     3000\na[83]     3000\na[84]     3000\na[85]     1200\na[86]     3000\nb         3000\nmu.a      1500\nsigma.a   3000\nsigma.y   3000\ny.tilde   3000\ny.tilde2  3000\ndeviance  3000\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 83.4 and DIC = 2177.2\nDIC is an estimate of expected predictive error (lower deviance is better).\n\n\nNow run the model and analyze the predicted radon level using the same approach as above.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>No-Pooling Regression with No Constant Term</span>"
    ]
  }
]