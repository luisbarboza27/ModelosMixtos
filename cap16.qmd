---
title: "Capitulo 16"
---

## Introduction to Multilevel Modeling in R and Bugs

Multilevel models allow intercepts and slopes to vary across groups. These models are useful when there is hierarchical data (e.g., individuals nested within schools, or measurements within counties).

R functions like lm() and lmer() can be used for classical and multilevel models, but they rely on point estimates and may have limitations, especially with small sample sizes or complicated models.

Bugs is introduced as a solution for complex multilevel models, enabling full Bayesian inference which accounts for uncertainty in parameter estimates.

### Key Steps in Multilevel Modeling:

1.  Start with classical regression using lm() or glm() in R.
2.  Add varying intercepts/slopes with lmer().
3.  Use JAGS or STAN to fit fully Bayesian models when needed.
4.  For very large models, further programming in R may be required.

## Bayesian inference and prior distributions

The main challenge in fitting a multilevel model is estimating both the data-level regression (which includes the coefficients for all group indicators) and the group-level model. The most direct approach is through Bayesian inference, which uses the group-level model as "prior information" to help estimate individual-level coefficients.

In Bayesian multilevel models, prior distributions are required for all parameters. These priors typically fall into two categories:

-   Group-level models, often represented by normal distributions, where the mean and standard deviation are assigned noninformative priors.

-   Noninformative uniform distributions, used when there is little prior information about the parameters.

### Classical Regression and Generalized Linear Models

In **classical regression** and **generalized linear models (GLMs)**, these models can be seen as special cases of multilevel modeling, where **no group-level model** is specified, meaning there is **no prior information** in the Bayesian sense. The key points are:

#### Classical Regression Model

In classical regression, the model is represented as:

$$
y_i = X_i \beta + \epsilon_i
$$

with independent errors:

$$
\epsilon_i \sim N(0, \sigma_y^2)
$$

The classical model assumes **no prior structure** on the parameters, where:

-   The **coefficients** $\beta$ have a uniform prior distribution over the entire real number range $(-\infty, \infty)$.
-   The **error variance** $\sigma_y$ has a uniform prior distribution on $(0, \infty)$.

#### Classical Logistic Regression Model

In **classical logistic regression**, the probability that $y_i = 1$ is given by:

$$
\text{Pr}(y_i = 1) = \text{logit}^{-1}(X_i \beta)
$$

Again, the prior on the components of $\beta$ is uniform, meaning there is no assumption or restriction on the parameter values.

### Simplest Varying-Intercept Model

The simplest form of multilevel regression is the **varying-intercept model**, where both individual and group-level errors are normally distributed. The model can be expressed as:

$$
y_i \sim N(\alpha_{j[i]} + \beta x_i, \sigma_y^2)
$$

and:

$$
\alpha_j \sim N(\mu_\alpha, \sigma_\alpha^2)
$$

#### Interpretation:

-   The group-specific intercepts $\alpha_j$ follow a normal distribution, which can be thought of as a **prior distribution** for the intercepts in a Bayesian framework.
-   The parameters $\mu_\alpha$ and $\sigma_\alpha$ are **hyperparameters** that describe the distribution of the group-level intercepts and are estimated from the data.

#### Prior Distributions in Bayesian Inference:

-   In **Bayesian inference**, all parameters, including hyperparameters $\mu_\alpha$, $\sigma_\alpha$, $\beta$, and $\sigma_y$, must have prior distributions.
-   Typically, these priors are set to **noninformative uniform distributions** when there is little prior knowledge about the parameters.

The complete prior distribution is written as:

$$
p(\alpha, \beta, \mu_\alpha, \sigma_y, \sigma_\alpha) \propto \prod_{j=1}^{J} N(\alpha_j | \mu_\alpha, \sigma_\alpha^2)
$$

This indicates that the intercepts ( \alpha\_j ) are modeled independently, following normal distributions, and their probability densities are multiplied to form the **joint prior density**.

### Varying-Intercept, Varying-Slope Model

A more complex form of multilevel modeling is the **varying-intercept, varying-slope model**, where both the intercepts and slopes vary across groups. The model is expressed as:

$$
y_i = \alpha_{j[i]} + \beta_{j[i]} x_i + \epsilon_i
$$

#### Model Parameters:

-   This model has $2J$ parameters, represented by $J$ pairs: $(\alpha_j, \beta_j)$.
-   Each group $j$ has its own **intercept** $\alpha_j$ and **slope** $\beta_j$, which vary across groups.

#### Prior Distributions:

-   The prior distribution for the pairs $(\alpha_j, \beta_j)$ is a **bivariate normal distribution**, allowing for correlation between the intercept and slope within each group.
-   The **hyperparameters** governing the distribution of $\alpha_j$ and $\beta_j$ (such as the means and variances) are typically assigned **independent uniform prior distributions** when no prior knowledge is available.

#### Joint Prior Distribution:

The joint prior for the intercepts and slopes across all groups can be expressed as a bivariate normal distribution, which captures the relationship between the group-level intercepts and slopes:

$$
(\alpha_j, \beta_j) \sim \text{Bivariate Normal}(\mu_{\alpha}, \mu_{\beta}, \Sigma)
$$

Where: - $\mu_{\alpha}$ and $\mu_{\beta}$ are the means of the intercepts and slopes. - $\Sigma$ is the covariance matrix, which captures the variances of the intercepts and slopes and the correlation between them.

### Multilevel Model with Group-Level Predictors: Exchangeability and Prior Distributions

In multilevel models, we typically **do not assign models to the coefficients** of group-level or individual-level predictors that do not vary by group. In Bayesian terminology, we often assign **noninformative uniform prior distributions** to these coefficients.

#### Group-Level Regression and Prior Distributions:

A **group-level regression** induces different prior distributions on the group-level coefficients. Consider the following simple varying-intercept model with one predictor at the individual level and one at the group level:

$$
y_i = \alpha_{j[i]} + \beta x_i + \epsilon_i, \ \epsilon_i \sim N(0, \sigma_y^2), \ \text{for} \ i = 1, \ldots, n
$$

$$
\alpha_j = \gamma_0 + \gamma_1 u_j + \eta_j, \ \eta_j \sim N(0, \sigma_\alpha^2), \ \text{for} \ j = 1, \ldots, J
$$

#### Model Interpretation:

-   The first equation is the **data model** or **likelihood**.
-   The second equation is the **group-level model** or **prior model** for the intercepts $\alpha_j$.
-   The intercepts $\alpha_j$ have different **prior distributions** based on the group-level predictor $u_j$. The mean of $\alpha_j$ is $\hat{\alpha_j} = \gamma_0 + \gamma_1 u_j$, and the standard deviation is $\sigma_\alpha$.

#### Exchangeability of the Group-Level Errors:

An equivalent way to think of this model is that the **group-level errors** $\eta_j$ are exchangeable:

$$
\alpha_j = \gamma_0 + \gamma_1 u_j + \eta_j, \ \eta_j \sim N(0, \sigma_\alpha^2)
$$

In this view, the $\alpha_j$'s are determined by the group-level predictor $u_j$ and the group-level error $\eta_j$, with $\eta_j$ assigned a common prior distribution.

#### Key Insights:

-   The **prior distribution** in a multilevel model can be viewed in two ways:
    1.  As a model representing a **group-level estimate** for each $\alpha_j$.
    2.  As a single model representing the distribution of the **group-level errors** $\eta_j$.

In practical applications, it is often more efficient to use the first approach when working with group-level predictors in software like **Bugs**, as this reduces the number of variables and speeds up the computation.

## Fitting and Understanding a Varying-Intercept Multilevel Model

### Loading Data in R

We use radon measurements and floor indicators (basement or first floor) for 919 homes in 85 counties in Minnesota. Since we assume multiplicative effects, we work with the **logarithms of radon levels**. Any radon measurements recorded as 0.0 are corrected to 0.1 before taking the logarithm.

```{r}
srrs2 <- read.table("./data/ARM_Data/radon/srrs2.dat", header=TRUE, sep=",")
mn <- srrs2$state == "MN"
radon <- srrs2$activity[mn]
y <- log(ifelse(radon == 0, 0.1, radon))
n <- length(radon)
x <- srrs2$floor[mn]  # 0 for basement, 1 for first floor
```

To account for county-specific effects, we create a county-level indicator for each observation:

```{r}
srrs2.fips <- srrs2$stfips * 1000 + srrs2$cntyfips
county.name <- as.vector(srrs2$county[mn])
uniq.name <- unique(county.name)
J <- length(uniq.name)
county <- rep(NA, J)
for (i in 1:J) {
  county[county.name == uniq.name[i]] <- i
}

```

### Classical Complete-Pooling Regression in R

We start by fitting a **classical regression model** that ignores county-level differences, treating the entire dataset as if it comes from a single population. This approach is known as **complete pooling**.

```{r}
# Complete-pooling model
lm.pooled <- lm(y ~ x)

# Display results
summary(lm.pooled)

```

### Classical No-Pooling Regression in R

In the **no-pooling model**, we include **county-specific indicators** to allow each county to have its own intercept. This model includes 85 counties but uses 84 indicators since we already have a constant term.

```{r}
# No-pooling model
lm.unpooled.0 <- lm(formula = y ~ x + factor(county))

# Display results
summary(lm.unpooled.0)
```

Interpretation:

-   County-specific intercepts represent differences from the reference (County 1).
-   For instance, log radon levels in County 2 are $0.03$ higher than County 1.
-   This model fits better than the complete-pooling model (with a lower residual SD and higher $R^2$).
-   However, estimates for individual counties may be uncertain, especially for smaller counties.

---
title: "No-Pooling Regression with No Constant Term"
output: html_document
---

### No-Pooling Regression with No Constant Term

To fit a model where each county has its own intercept without a constant term, we use the `-1` in the formula. This allows each county to have its own intercept, making predictions for individual counties more convenient.

```{r}
# No-pooling model with no constant term
lm.unpooled <- lm(formula = y ~ x + factor(county) - 1)

# Display results
summary(lm.unpooled)
```

Interpretation:

-   County-specific intercepts are provided for all 85 counties.
-   The R-squared appears inflated ($0.77$) compared to the no-pooling model ($0.29$), but this is due to how the lm() function calculates explained variance without a constant term.
-   The estimates for intercepts are consistent with the previous parameterization.

## Setting up a Multilevel Regression Model in JAGS

We can set up the multilevel model for the radon problem using **JAGS** (Just Another Gibbs Sampler) instead of **Bugs**. Below is the JAGS model code for a varying-intercept multilevel model.

### JAGS Model Code:

See radon_model.jags for the JAGS model code.

Explanation:

-   Likelihood: Each observation's radon level, y\[i\], is modeled as normally distributed with mean y.hat\[i\], where y.hat\[i\] = a\[county\[i\]\] + b \* x\[i\].
-   Priors:
    -   The slope b is given a noninformative prior (dnorm(0, 0.0001)).
    -   The group-level intercepts a\[j\] are also normally distributed with mean mu.a and precision tau.a.
    -   sigma.y and sigma.a are assigned uniform priors.

### Running JAGS in R:

Once the model is set up, we can run it in R using the rjags package. Below is the R code to load the data, initialize the model, and run it:

```{r}
# Load the rjags package
library(rjags)

# Prepare data list for JAGS
data_jags <- list(y = y, x = x, county = county, n = n, J = J)

# Initial values for JAGS
inits <- function() {
  list(a = rnorm(J), b = rnorm(1), mu.a = rnorm(1), sigma.y = runif(1), sigma.a = runif(1))
}

# Parameters to monitor
params <- c("a", "b", "mu.a", "sigma.y", "sigma.a")

# Run JAGS
jags_model <- jags.model("codigoJAGS/radon_model.jags", data = data_jags, inits = inits, n.chains = 3)
update(jags_model, 1000)  # Burn-in
samples <- coda.samples(jags_model, variable.names = params, n.iter = 5000)
summary(samples)
```

Diagnósticos:

```{r}
print(jags_model)
```

```{r}
library(R2jags)
set.seed(1234)
jags_model2 <-
  jags(
    model.file = "codigoJAGS/radon_model.jags", data = data_jags,parameters.to.save = c("a", "b", "mu.a", "sigma.y", "sigma.a"),n.iter = 10000, n.burnin = 3000,n.thin = 2)
```

```{r}
jags_model2
```

```{r}
library(CalvinBayes)
diagMCMC(samples , parName="a[1]" )
diagMCMC(samples , parName="sigma.a" )
```

In a Bayesian analysis run using Bugs or JAGS, the **summary** typically includes the **means, standard deviations, and quantiles** of all parameters, along with two key diagnostics for convergence:

1.  **R-hat** (Potential Scale Reduction Factor): This measures how much the parameter estimates would improve if the model were run indefinitely. **R-hat ≤ 1.1** indicates good convergence.

2.  **Effective Sample Size (n_eff)**: This metric reflects how many independent samples the Markov Chain Monte Carlo (MCMC) has effectively produced. A high **n_eff** value means the parameter estimates are reliable and not highly autocorrelated, leading to better precision. We usually like to have **n_eff** to be at least 100 for typical estimates and confidence intervals.

In JAGS, after running your model, the simulations (posterior samples) can be accessed in R. The saved MCMC object contains simulation draws for each parameter. For example, scalar parameters like b, mu.a, sigma.y, and sigma.a are vectors of length corresponding to the number of saved draws (e.g., 750). Parameters like a (which varies by county) will be represented as matrices.

Example code for accessing and summarizing parameter estimates:

```{r}
#samples <- coda.samples(jags_model2, variable.names = #params, n.iter = 5000)

# Quantiles for b
quantile(as.numeric(samples[[1]][,"b"]), c(0.05, 0.95))

# Probability that radon levels in county 36 are higher than in county 26
mean(samples[[1]][,'a[36]'] > samples[[1]][,'a[26]'])
```

Fitted Values, Residuals, and Other Calculations in JAGS

In JAGS, you can calculate fitted values and residuals after running the model by combining parameter samples with the observed data. For instance:

```{r}
# Calculate fitted values and residuals
## y.hat <- a.multilevel[county] + b.multilevel * x
## y.resid <- y - y.hat

# Plot residuals
## plot(y.hat, y.resid)
```

You can also add y.hat as a monitored parameter during the JAGS run. For predictive checks, such as comparing radon levels between two counties:

```{r}
b_samples <- as.numeric(samples[[1]][,"b"])
a36_samples <- as.numeric(samples[[1]][,"a[36]"])
a26_samples <- as.numeric(samples[[1]][,"a[26]"])
sigma.y <- as.numeric(samples[[1]][,"sigma.y"])
# Predictive distribution for radon levels
lqp.radon <- exp(rnorm(n = 5000, a36_samples + b_samples, sigma.y))
hennepin.radon <- exp(rnorm(n = 5000, rnorm(1, a26_samples + b_samples, sigma.y)))
radon.diff <- lqp.radon - hennepin.radon
hist(radon.diff)
```

## Adding Individual- and Group-Level Predictors in JAGS

In JAGS, we can add both individual- and group-level predictors to the model. For complete-pooling regression, where no group-level variation is considered, a simple linear regression can be fit using the following JAGS model:

```{r}
## JAGS model for complete pooling
# model {
#   for (i in 1:n) {
#     y[i] ~ dnorm(y.hat[i], tau.y)
#     y.hat[i] <- a + b * x[i]  # Complete-pooling regression
#   }
#   
#   # Priors
#   a ~ dnorm(0, 0.0001)
#   b ~ dnorm(0, 0.0001)
#   tau.y <- pow(sigma.y, -2)
#   sigma.y ~ dunif(0, 100)
# }

```

Explanation:

-   y\[i\] is the radon level for house i.
-   x\[i\] is the basement status (individual-level predictor).
-   a is the intercept, and b is the slope.

No-Pooling Model in JAGS

In a no-pooling model, the intercepts for each county are allowed to vary, but no information is shared between them (i.e., no hierarchical structure). The JAGS model allows for county-specific intercepts (a\[county\[i\]\]) while maintaining a common slope (b).

```{r}
# # JAGS model for no-pooling
# model {
#   for (i in 1:n) {
#     y[i] ~ dnorm(y.hat[i], tau.y)
#     y.hat[i] <- a[county[i]] + b * x[i]  # No-pooling model
#   }
#   
#   # Priors
#   b ~ dnorm(0, 0.0001)
#   tau.y <- pow(sigma.y, -2)
#   sigma.y ~ dunif(0, 100)
#   
#   for (j in 1:J) {
#     a[j] ~ dnorm(0, 0.0001)  # County-specific intercepts
#   }
# }
```

Explanation:

-   a\[j\]: intercept for county j.
-   b: common slope for the basement status.

Classical Regression with Multiple Predictors in JAGS

In a classical regression with multiple predictors, you can extend the model to include additional covariates like whether the radon measurement was taken in winter. The model can also include interaction terms.

```{r}
# # JAGS model for classical regression with multiple predictors
# model {
#   for (i in 1:n) {
#     y[i] ~ dnorm(y.hat[i], tau.y)
#     y.hat[i] <- a + b[1]*x[i] + b[2]*winter[i] + b[3]*x[i]*winter[i]  # Multiple predictors and interaction
#   }
#   
#   # Priors for regression coefficients
#   for (k in 1:K) {
#     b[k] ~ dnorm(0, 0.0001)
#   }
#   
#   # Other priors
#   a ~ dnorm(0, 0.0001)
#   tau.y <- pow(sigma.y, -2)
#   sigma.y ~ dunif(0, 100)
# }

```

Explanation:

-   b\[1\], b\[2\], b\[3\]: coefficients for x, winter, and their interaction.
-   a: intercept.
-   winter\[i\]: an indicator for whether the measurement was taken in winter.

Vector-Matrix Notation in JAGS

To efficiently handle multiple predictors in JAGS, you can use vector-matrix notation. First, in R, create a matrix of predictors:

```{r}
# # Create a matrix of predictors in R
# X <- cbind(x, winter, x * winter)
# K <- ncol(X)

```

Then, in the JAGS model, use the inner-product function for the linear predictor:

```{r}
# # JAGS model with vector-matrix notation
# model {
#   for (i in 1:n) {
#     y[i] ~ dnorm(y.hat[i], tau.y)
#     y.hat[i] <- a + inprod(b[], X[i,])
#   }
# 
#   # Priors for regression coefficients
#   for (k in 1:K) {
#     b[k] ~ dnorm(0, 0.0001)
#   }
# 
#   # Other priors
#   a ~ dnorm(0, 0.0001)
#   tau.y <- pow(sigma.y, -2)
#   sigma.y ~ dunif(0, 100)
# }
```

If you want to include the intercept in the matrix, prepend a vector of ones to the predictor matrix:

```{r}
# # Include intercept in the predictor matrix
# ones <- rep(1, n)
# X <- cbind(ones, x, winter, x * winter)
# K <- ncol(X)

```

In the JAGS model, simplify y.hat by using inprod(b\[\], X\[i,\]), where the coefficients b\[1\], ..., b\[4\] correspond to the intercept and other predictors. This approach makes handling multiple predictors more efficient.

Multilevel Model with a Group-Level Predictor in JAGS

In this multilevel model, a group-level predictor (such as uranium levels) influences the county-specific intercepts. Here’s the equivalent JAGS model:

Preparación de datos:

```{r}
srrs2.fips <- srrs2$stfips*1000 + srrs2$cntyfips
cty <- read.table ("data/ARM_Data/radon/cty.dat", header=T, sep=",")
usa.fips <- 1000*cty[,"stfips"] + cty[,"ctfips"]
usa.rows <- match (unique(srrs2.fips[mn]), usa.fips)
uranium <- cty[usa.rows,"Uppm"]
u <- log (uranium)

u.full <- u[county]
```

```{r}
data_jags$u <- u.full

jags_model2_u <-
  jags(
    model.file = "codigoJAGS/radon_model_u.jags", data = data_jags,parameters.to.save = c("a", "b", "sigma.y", "sigma.a","g.0","g.1"),n.iter = 3000, n.burnin = 1000,n.thin = 2)


```

```{r}
jags_model2_u
```

Explanation:

-   u\[j\]: group-level predictor (e.g., county-level uranium).
-   a\[j\]: county-specific intercept influenced by the group-level predictor.
-   g.0, g.1: regression coefficients for the group-level predictor.

This model accounts for both individual- and group-level variations.

Predictions for New Observations and New Groups in JAGS

Predicting a New Unit in an Existing Group:

To predict for a new house in an existing group, extend the dataset in R by adding an NA for the new observation. For example, to predict radon levels in a new house in county 26 without a basement:

```{r}
# Extend dataset
n <- n + 1
y <- c(y, NA)
county <- c(county, 26)
x <- c(x, 1)

```

Add the following line in the JAGS model to flag the predicted value:

```{r}
# y.tilde <- y[n]

```

Then run the model, saving the predicted value:

```{r}
data_jags_pred <- list(y = y, x = x, county = county, n = n, J = J)

jags_model3 <-
  jags(
    model.file = "codigoJAGS/radon_model_pred1.jags", data = data_jags_pred,parameters.to.save = c("a", "b", "mu.a", "sigma.y", "sigma.a","y.tilde"),n.iter = 3000, n.burnin = 1000,n.thin = 2)


# radon.parameters <- c(radon.parameters, "y.tilde")

```

To summarize predictions:

```{r}
jags_model3_mcmc <- as.mcmc(jags_model3)

diag_mcmc(jags_model3_mcmc, parName = "y.tilde")


quantile(exp(jags_model3_mcmc[,"y.tilde"][[1]]), c(0.25, 0.75))  # 50% confidence interval

```

Predicting a New Unit in a New Group:

For predictions in a new group (e.g., a new county with no previous radon data), you can add a new county to the dataset. First, define the group-level predictor (e.g., average uranium level):

```{r}
u.tilde <- mean(u)  # Group-level predictor for new county

```

Then extend the dataset for the new group:

```{r}
n <- n + 1
y <- c(y, NA)
county <- c(county, J + 1)
x <- c(x, 1)
J <- J + 1
u <- c(u, u.tilde)

data_jags_pred2 <- list(y = y, x = x, county = county, n = n, J = J, u = u)

jags_model4 <-
  jags(
    model.file = "codigoJAGS/radon_model_pred2.jags", data = data_jags_pred2,parameters.to.save = c("a", "b", "mu.a", "sigma.y", "sigma.a","y.tilde","y.tilde2"),n.iter = 3000, n.burnin = 1000,n.thin = 2)
```

```{r}
jags_model4
```

Now run the model and analyze the predicted radon level using the same approach as above.
